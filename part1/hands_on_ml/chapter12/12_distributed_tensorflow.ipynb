{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12 – Distributed TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11장에서는 훈련 속도를 개선하기 위한 여러 기술을 논의했다:\n",
    "- 가중치 초기화\n",
    "- Batch Normalization\n",
    "- sophisticated optimizer\n",
    "\n",
    "하지만, 단일 machine에 단일 CPU에서 큰 nn을 학습하는 데는 수일, 수주가 걸릴 수 있다\n",
    "\n",
    "이번 장에서 우리는 다중 device(CPUs and GPUS)에 걸쳐 분산 컴퓨팅 환경에서 TensorFlow를 병렬로 구동하는 방법을 배울 것이다. [Fig 12-1]\n",
    "![](https://i.imgur.com/KAcAtXa.png)\n",
    "\n",
    "1. 먼저, [Fig 12-1]과 같이 단일 머신위 다중 devices에서의 분산 계산을 수행하고나서,\n",
    "2. 다중 머신위 다중 devices에서 분산 계산을 수행할 것이다.\n",
    "\n",
    "TensorFlow의 분산 컴퓨팅은 경쟁하는 다른 nn F/W과 비교해 조명을 받는 것 중 하나다:\n",
    "- 계산 graph를 device와 server 별로 쪼게고, 복제하기 위한 완벽한 제어를 할 수 있다.\n",
    "- 유연하게 tensor ops를 병렬처리하고 동기화하여, 모든 종류의 병렬화 접근방법을 선택할 수 있다.\n",
    "\n",
    "nn의 훈련과 실행을 병렬화하는 가장 인기있는 접근방법 중 몇가지를 통해:\n",
    "- 몇주가 걸릴 수 있는 훈련을 단 몇시간에 끝낼 수 있으며,\n",
    "- 시간 절약 뿐아니라, 다양한 모델을 실험할 수 있고\n",
    "- 좀더 신선한 data로 훈련시킨 최신의 모델을 유지할 수 있다.\n",
    "- fine-tuning시 더 많은 hyperparamer를 탐색하고, 더 많은 앙상블을 사용할 수 있다.\n",
    "\n",
    "그러나 뛰기 전에 먼저 걷자. 단일 머신에 다중 CPU를 달고 간단한 graph로 시작해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.0 Setup\n",
    "먼저 python2, 3에서 notebook이 잘 작동하지는 확인하기 위해 몇 공통 모듈을 import해보고 MatplotLib로 그린 그림을 notebook에 그리고, 저장할 수 있도록하자:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"distributed\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 단일 머신 위 다중 devices\n",
    "단일 머신에 단지 GPU 하나 추가해서 놀라운 성능향상을 맛볼 수 있다. 사실상 많은 경우 이것만으로도 충분하다. 예를 들면, 다중 머신 위 16GPU보다 단일 머신 8GPU가 더 빠를 수 있다.(network 전송이 발목을 잡는다.)\n",
    "\n",
    "이번 절에서는 이러한 환경에서 TensorFlow의 환경을 설정하고, 가용 device들에 ops를 분산 배치하여 이를 병렬로 처리하는 방법을 배울 것이다.\n",
    "\n",
    "### 12.1.1 설치\n",
    "1) 다중 GPU 카드에서 TensorFlow를 구동하기 위해서는: \n",
    "- GPU들의 NVidia Comput Capability가 3 이상이어야 하며,\n",
    "- 이러한 카드들은 Nvidia의 Titan, Titan X, K20, K40카드가 있으며,\n",
    "- [여기](https://developer.nvidia.com/cuda-gpus)에서 다른 카드들의 Capability를 확인할 수 있다.\n",
    "\n",
    ">GPU가 없다면 클라우드 서비스 업체로부터 호스팅 서비스를 받을 수 있다:\n",
    "- 아마존 AWS를 이용하는 방법은 http://goo.gl/kbge5b 참고\n",
    "- 구글에서는 [Cloud Machine Learning](https://cloud.google.com/ml)을 이용할 수 있으며,\n",
    "- GPU 구매는 정기적으로 업데이트되는 [Time Dettets의 blog](https://goo.gl/pCtSAn)을 참고하라.\n",
    "\n",
    "2) Nvidia's Deep Learning SDK 관련 라이브러리 설치:\n",
    "TensorFlow는 GPU 카드를 제어하고 고속 연산을 위해 CUDA와 cuDNN을 필요로 한다.\n",
    "- CUDA(Compute Unifed Device Architecture) : tensorflow 1.3에서는 v8.0\n",
    "- cuDNN(CUDA Deep Neural Network) library: v5.1이 필요\n",
    "\n",
    "![](https://i.imgur.com/lGPiLLI.png)\n",
    "\n",
    "nvidia-smi 명령어로 CUDA가 적절히 설치되었는지 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  5 14:59:22 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN X (Pascal)    Off  | 0000:05:00.0      On |                  N/A |\r\n",
      "| 23%   39C    P8    11W / 250W |    149MiB / 12181MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN X (Pascal)    Off  | 0000:06:00.0     Off |                  N/A |\r\n",
      "| 23%   40C    P8    10W / 250W |      1MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN X (Pascal)    Off  | 0000:09:00.0     Off |                  N/A |\r\n",
      "| 23%   37C    P8    11W / 250W |      1MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  TITAN X (Pascal)    Off  | 0000:0A:00.0     Off |                  N/A |\r\n",
      "| 23%   31C    P8     9W / 250W |      1MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     14174    G   /usr/bin/X                                     146MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가용 GPU list와 각 카드에서 구동되는 process들을 보여준다.\n",
    "\n",
    "3) CUDA와 cuDNN의 경로를 환경변수로 설정한다.\n",
    "- MacOSX에서는 다음 코드를 .bash_profile에 추가한다.\n",
    "```bash\n",
    "export CUDA_HOME=/usr/local/cuda\n",
    "export PATH=$PATH:$CUDA_HOME/bin\n",
    "export CUDA_LIBS=$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64\n",
    "export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:$CUDA_LIBS\n",
    "```\n",
    "- Linux에서는 DYLD_LIBRARY_PATH을 LD_LIBRARY_PATH로 바꾼다.\n",
    "```bash\n",
    "export CUDA_HOME=/usr/local/cuda\n",
    "export PATH=$PATH:$CUDA_HOME/bin\n",
    "export CUDA_LIBS=$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_LIBS\n",
    "```\n",
    "\n",
    "4) 마지막으로 TensorFlow GPU 버전을 설치한다.\n",
    "- TensorFlow 홈페이지를 참고하자.\n",
    "\n",
    "이제 모든 설치가 끝났으니 python shell에서 TensorFlow를 인식하고 CUDA와 cuDNN을 적절히 이용하는 지 확인해보자.\n",
    "\n",
    "#### local\n",
    "python shell에서 실행할 때 device를 모두 확인해볼 수 있다.\n",
    "```python\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "```\n",
    "출력 log를 보면:\n",
    "```bash\n",
    "...\n",
    "2017-09-05 15:42:47.962202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:0a:00.0)\n",
    "2017-09-05 15:42:47.962213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:09:00.0)\n",
    "2017-09-05 15:42:47.962221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)\n",
    "2017-09-05 15:42:47.962228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)\n",
    "```\n",
    "- TensorFlow가 CUDA와 cdDNN 라이브러리를 검출하고, \n",
    "- GPU 카드를 검출하기 위해 CUDA 라이브러리를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tf.constant(\"Hello distributed TensorFlow!\")\n",
    "server = tf.train.Server.create_local_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello distributed TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(server.target) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.2 GPU 램 관리하기\n",
    "디폴트로 TensorFlow는 처음 graph를 구동할 때, 자동으로 가용 GPU의 모든 RAM을 차지하기 때문에, 첫 프로그램이 종료하기 전까지 두번째 프로그램을 구동할 수 없다.\n",
    "\n",
    "이때, 두번째 프로그램을 구동하면 다음과 같은 error가 발생한다:\n",
    "```bash\n",
    "E [...]/cuda_driver.cc:965] failed to allocate 3.66G (3928915968 bytes) from\n",
    "device: CUDA_ERROR_OUT_OF_MEMORY\n",
    "```\n",
    "\n",
    "이에 대한 해결방안은:\n",
    "\n",
    "1) 사용가능한 GPU를 지시:\n",
    "- 단순히 CUDA_VISIBLE_DEVICES 환경 변수를 설정한다.\n",
    "- 예를 들면 program을 아래와 같이 시작한다.\n",
    "```bash\n",
    "$ CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py\n",
    "# and in another terminal:\n",
    "$ CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py\n",
    "```\n",
    "    - 프로그램 1은 GPU 카드 0과 1만 인식하고(프로그램 내부에서는 0, 1로 번호매김)\n",
    "    - 프로그램 2는 GPU 카드 2와 3만 인식한다.(프로그램 내부에서는 0, 1로 번호매김)\n",
    "\n",
    "![](https://i.imgur.com/K4jb76J.png)\n",
    "\n",
    "2) 메모리의 부분 사용을 지시:\n",
    "- ConfigProto 객체를 생성하여, 각 GPU 메모리의 40%만 사용하도록할 수 있다.\n",
    "```bash\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "```\n",
    "\n",
    "![](https://i.imgur.com/VkfGB2a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-05 16:34:21.623756: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:21.623786: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:21.623795: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:21.623801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:21.623808: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:21.935567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:0a:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.75GiB\n",
      "2017-09-05 16:34:21.935646: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x24f2ae0\n",
      "2017-09-05 16:34:22.087430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:09:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.75GiB\n",
      "2017-09-05 16:34:22.088598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \n",
      "2017-09-05 16:34:22.088614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y \n",
      "2017-09-05 16:34:22.088621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y \n",
      "2017-09-05 16:34:22.088636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:0a:00.0)\n",
      "2017-09-05 16:34:22.088645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:09:00.0)\n",
      "2017-09-05 16:34:22.157203: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -> {0 -> localhost:46171}\n",
      "2017-09-05 16:34:22.158259: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:46171\n",
      "2017-09-05 16:34:27.165052: I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session 009beb62c6350b9f with config: \n",
      "\n",
      "Hello distributed TensorFlow 1!\n",
      "2017-09-05 16:34:28.221298: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:28.221328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:28.221337: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:28.221343: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:28.221350: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:34:28.454526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:06:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.75GiB\n",
      "2017-09-05 16:34:28.454722: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2cd8e70\n",
      "2017-09-05 16:34:28.656523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:05:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.60GiB\n",
      "2017-09-05 16:34:28.657636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \n",
      "2017-09-05 16:34:28.657652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y \n",
      "2017-09-05 16:34:28.657659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y \n",
      "2017-09-05 16:34:28.657673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)\n",
      "2017-09-05 16:34:28.657683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)\n",
      "2017-09-05 16:34:28.725538: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -> {0 -> localhost:38962}\n",
      "2017-09-05 16:34:28.726644: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:38962\n",
      "2017-09-05 16:34:33.728694: I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session d4222bbc9c40a985 with config: \n",
      "\n",
      "Hello distributed TensorFlow 2!\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 python ../program1.py\n",
    "!CUDA_VISIBLE_DEVICES=2,3 python ../program2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf\r\n",
      "import time\r\n",
      "\r\n",
      "c = tf.constant(\"Hello distributed TensorFlow 3!\")\r\n",
      "server = tf.train.Server.create_local_server()\r\n",
      "\r\n",
      "time.sleep(5)\r\n",
      "\r\n",
      "config = tf.ConfigProto()\r\n",
      "config.gpu_options.per_process_gpu_memory_fraction = 0.4\r\n",
      "\r\n",
      "with tf.Session(server.target, config=config) as sess:\r\n",
      "\tprint(sess.run(c))\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../program3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-05 16:46:32.745415: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:46:32.745443: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:46:32.745451: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:46:32.745457: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:46:32.745463: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 16:46:33.072614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:0a:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.75GiB\n",
      "2017-09-05 16:46:33.072710: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x126dd00\n",
      "2017-09-05 16:46:33.280668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:09:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.75GiB\n",
      "2017-09-05 16:46:33.280756: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2d31b90\n",
      "2017-09-05 16:46:33.470672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 2 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:06:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.75GiB\n",
      "2017-09-05 16:46:33.470762: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2d35510\n",
      "2017-09-05 16:46:33.698571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 3 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:05:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 11.60GiB\n",
      "2017-09-05 16:46:33.704574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 \n",
      "2017-09-05 16:46:33.704594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y Y Y \n",
      "2017-09-05 16:46:33.704601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y Y Y \n",
      "2017-09-05 16:46:33.704608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   Y Y Y Y \n",
      "2017-09-05 16:46:33.704614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   Y Y Y Y \n",
      "2017-09-05 16:46:33.704631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:0a:00.0)\n",
      "2017-09-05 16:46:33.704641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:09:00.0)\n",
      "2017-09-05 16:46:33.704649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)\n",
      "2017-09-05 16:46:33.704655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)\n",
      "2017-09-05 16:46:33.852581: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -> {0 -> localhost:51402}\n",
      "2017-09-05 16:46:33.854036: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:51402\n",
      "2017-09-05 16:46:38.861162: I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session 71015285cfc2ed51 with config: \n",
      "gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.4\n",
      "}\n",
      "\n",
      "Hello distributed TensorFlow 3!\n"
     ]
    }
   ],
   "source": [
    "!python ../program3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">만약 RAM 40%를 사용하는 3개의 프로그램이 가동되면:\n",
    "- 3 * .4 > 1 이므로, 3번째 프로그램은 error가 발생한다.\n",
    "\n",
    ">프로그램 구동 중, nvidia-smi 명령어를 실행하면, 각 카드의 40%의 메모리를 사용하는 것을 볼 수 있다.\n",
    "\n",
    "3) 다른 옵션은 TensorFlow에게 필요할 때만 RAM을 제어하도록 할 수 있다.\n",
    "- config.gpu_options.allow_growth=True로 설정.\n",
    "- 그러나 실제로 Tensorflow는 메모리 파편화를 피하기 위해 한번 차지한 RAM을 release하지 않는다.\n",
    "- 따라서 별로 권장하는 옵션은 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3 devices에 ops을 할당\n",
    "TensorFlow 백서에는 친숙한 dynamic placer 알고리즘을 제공한다:\n",
    "- 다음 사항들을 고려하여:\n",
    "    - graph의 이전 수행에서 측정된 연산시간 등, \n",
    "    - input size 추정치, \n",
    "    - 각 ops의 출력 tensor,\n",
    "    - 각 devices의 가용 메모리 량,\n",
    "    - devices 들간의 data 입출력에 따른 전송 지연\n",
    "    - 사용자로 부터의 제약사항 등\n",
    "- 모든 가용 devices에 자동적으로 ops를 분산 배치한다.\n",
    "- 불행히도, 이런 현한적인 알고리즘은 open source로 제공된 tensorflow에서는 공개되지 않았다.\n",
    "- 추론하기를 small set에 대한 배치는 dynamic placer보다 사용자가 지정하는 것이 더 효율적이라 생각된다.\n",
    "\n",
    "google의 TensorFlow 팀은 이 알고리즘을 개선하고 있으며, 언제가는 오픈할 것으로 예상한다.\n",
    "\n",
    "#### Simple placement\n",
    "graph를 구동할 때마다, TensorFlow는 아직 device들에 배치되지 않은 node들을 평가할 필요가 있다면, 단순 배치를 사용한다.\n",
    "\n",
    "단순 배치자는 다음 규칙을 따른다:\n",
    "- node가 graph의 이전 반복에서 어떤 device에 배치되었다면, 해당 device에 남겨둔다.\n",
    "- 그렇지 않고, 사용자가 특정 device에 그 node를 배치해다면, 그렇게 한다.\n",
    "- 그렇지 않다면, default로 GPU #0 또는 CPU(GPU가 없다면)에 배치한다.\n",
    "\n",
    "따라서 특정 ops를 적절한 device에 배치하는 것은 전적으로 사용자에 달려있다.\n",
    "- 아무것도 하지 않으면, 전체 graph는 default device에만 배치된다.\n",
    "\n",
    "특정 device에 node들을 지정하려면 device() 함수를 이용하여 device block을 생성해야 한다:\n",
    "- 다음 코드는 변수 a와 상수 b를 CPU에 배치하고,\n",
    "- 곱 node C는 아무 device에도 배치하지 않았다.\n",
    "- 따라서 c는 default device에 배칠될 것이다.\n",
    "```python\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "c = a * b\n",
    "```\n",
    "\n",
    ">\"/cpu:0\" device는 다중 CPU 시스템에 있는 모든 CPU들을 aggregate한다.\n",
    "- 아직까지는 특정 CPU에 배치하거나, CPUs의 subet에 배치할 방법은 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    a,b = tf.Variable(3.0), tf.Variable(4.0)\n",
    "c = a*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging placements\n",
    "사용자가 지정한 배치에 따라 단순 배치자가 동작하는 지 확인해보자:\n",
    "- 이를 위해 log_device_placement 옵션을 True로 설정한다.\n",
    "- 이는 배치자가 node를 배치할 때 log를 남기도록 한다.\n",
    "\n",
    "다음과 같은 logPlace.py을 생성하고 이를 실행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf\r\n",
      "\r\n",
      "with tf.device(\"/cpu:0\"):\r\n",
      "\ta = tf.Variable(3.0)\r\n",
      "\tb = tf.constant(4.0)\r\n",
      "\r\n",
      "c = a * b\r\n",
      "\r\n",
      "config = tf.ConfigProto()\r\n",
      "config.log_device_placement = True\r\n",
      "init = tf.global_variables_initializer()\r\n",
      "\r\n",
      "sess = tf.Session(config=config)\r\n",
      "\r\n",
      "init.run(session=sess)\r\n",
      "\r\n",
      "res = sess.run(c)\r\n",
      "print (res)\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../logPlace.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-05 18:31:14.448522: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 18:31:14.448551: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 18:31:14.448559: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 18:31:14.448567: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 18:31:14.448573: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-09-05 18:31:14.614222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:0a:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 419.50MiB\n",
      "2017-09-05 18:31:14.614303: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2bff1d0\n",
      "2017-09-05 18:31:14.768491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:09:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 451.50MiB\n",
      "2017-09-05 18:31:14.768731: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2fc1f20\n",
      "2017-09-05 18:31:14.955783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 2 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:06:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 451.50MiB\n",
      "2017-09-05 18:31:14.955857: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x3018b80\n",
      "2017-09-05 18:31:15.125905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 3 with properties: \n",
      "name: TITAN X (Pascal)\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.531\n",
      "pciBusID 0000:05:00.0\n",
      "Total memory: 11.90GiB\n",
      "Free memory: 444.38MiB\n",
      "2017-09-05 18:31:15.126670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 \n",
      "2017-09-05 18:31:15.126684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y Y Y \n",
      "2017-09-05 18:31:15.126691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y Y Y \n",
      "2017-09-05 18:31:15.126698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   Y Y Y Y \n",
      "2017-09-05 18:31:15.126704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   Y Y Y Y \n",
      "2017-09-05 18:31:15.126718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:0a:00.0)\n",
      "2017-09-05 18:31:15.126727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:09:00.0)\n",
      "2017-09-05 18:31:15.126734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)\n",
      "2017-09-05 18:31:15.126741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:0a:00.0\n",
      "/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: TITAN X (Pascal), pci bus id: 0000:09:00.0\n",
      "/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: TITAN X (Pascal), pci bus id: 0000:06:00.0\n",
      "/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: TITAN X (Pascal), pci bus id: 0000:05:00.0\n",
      "2017-09-05 18:31:15.133385: I tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:\n",
      "/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:0a:00.0\n",
      "/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: TITAN X (Pascal), pci bus id: 0000:09:00.0\n",
      "/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: TITAN X (Pascal), pci bus id: 0000:06:00.0\n",
      "/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: TITAN X (Pascal), pci bus id: 0000:05:00.0\n",
      "\n",
      "Variable: (VariableV2): /job:localhost/replica:0/task:0/cpu:0\n",
      "2017-09-05 18:31:15.133985: I tensorflow/core/common_runtime/simple_placer.cc:841] Variable: (VariableV2)/job:localhost/replica:0/task:0/cpu:0\n",
      "Variable/read: (Identity): /job:localhost/replica:0/task:0/cpu:0\n",
      "2017-09-05 18:31:15.134002: I tensorflow/core/common_runtime/simple_placer.cc:841] Variable/read: (Identity)/job:localhost/replica:0/task:0/cpu:0\n",
      "mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\n",
      "2017-09-05 18:31:15.134018: I tensorflow/core/common_runtime/simple_placer.cc:841] mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\n",
      "Variable/Assign: (Assign): /job:localhost/replica:0/task:0/cpu:0\n",
      "2017-09-05 18:31:15.134028: I tensorflow/core/common_runtime/simple_placer.cc:841] Variable/Assign: (Assign)/job:localhost/replica:0/task:0/cpu:0\n",
      "init: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
      "2017-09-05 18:31:15.134038: I tensorflow/core/common_runtime/simple_placer.cc:841] init: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
      "Const: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
      "2017-09-05 18:31:15.134049: I tensorflow/core/common_runtime/simple_placer.cc:841] Const: (Const)/job:localhost/replica:0/task:0/cpu:0\n",
      "Variable/initial_value: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
      "2017-09-05 18:31:15.134059: I tensorflow/core/common_runtime/simple_placer.cc:841] Variable/initial_value: (Const)/job:localhost/replica:0/task:0/cpu:0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "!python ../logPlace.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교재와 log가 좀 다르지만, 변수에 대한 device 매핑정보에서 'cpu'로 할당된 것을 확인할 수 있다.\n",
    "\n",
    "우리가 session을 생성할 때, TensorFlow는 GPU 카드를 발견했다는 정보를 log로 알려준다.\n",
    "- graph를 처음 구동할 때(a를 초기화할 때) 단순 배치자가 구동되고\n",
    "    - 배치자는 각 node를 할당한 device에 배치한다.\n",
    "    - 그러나, mul로 name된 node는 default device인 '/gpu:0'에 배치되었다.\n",
    "- graph를 다음 2번 째로 구동할 때(c를 계산할 때)\n",
    "    - c 계산을 위한 모든 node들이 이미 배치었기 때문에\n",
    "    - 단순 배치자는 더이상 사용되지 않는다.\n",
    "\n",
    ">\"/job:localhost/replica:0/task:0\"와 같은 prefix는 제거할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동적할당 함수\n",
    "device block을 생성할 때, device 명 대신 함수를 지정할 수 있다.\n",
    "- TF는 이 device block 내에 배치할 각 ops에 대해 이 함수를 호출하고,\n",
    "- 이 함수는 ops에 부착할 device 명을 리턴한다.\n",
    "\n",
    "예로 다음 코드는 모든 변수 노드를 \"/cpu:0\"에 배치하고, 그외 다른 노드는 \"/gpu:0\"에 배치한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variables_on_cpu(op):\n",
    "    if op.type == \"Variable\":\n",
    "        return \"/cpu:0\"\n",
    "    else:\n",
    "        return \"/gpu:0\"\n",
    "with tf.device(variables_on_cpu):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "    c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 예로부터 좀 더 복잡한 방식(각 변수를 round-robin 방식)으로 배치할 수 있다.\n",
    "\n",
    "#### Operations and kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster: Multi-Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_spec = tf.train.ClusterSpec({\n",
    "    \"ps\": [\n",
    "        \"127.0.0.1:2221\",  # /job:ps/task:0\n",
    "        \"127.0.0.1:2222\",  # /job:ps/task:1\n",
    "    ],\n",
    "    \"worker\": [\n",
    "        \"127.0.0.1:2223\",  # /job:worker/task:0\n",
    "        \"127.0.0.1:2224\",  # /job:worker/task:1\n",
    "        \"127.0.0.1:2225\",  # /job:worker/task:2\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task_ps0 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=0)\n",
    "task_ps1 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=1)\n",
    "task_worker0 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)\n",
    "task_worker1 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=1)\n",
    "task_worker2 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinning operations across devices and servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(\"/job:ps\"):\n",
    "    a = tf.Variable(1.0, name=\"a\")\n",
    "\n",
    "with tf.device(\"/job:worker\"):\n",
    "    b = a + 2\n",
    "\n",
    "with tf.device(\"/job:worker/task:1\"):\n",
    "    c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(\"grpc://127.0.0.1:2221\") as sess:\n",
    "    sess.run(a.initializer)\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(tf.train.replica_device_setter(\n",
    "        ps_tasks=2,\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:worker\")):\n",
    "    v1 = tf.Variable(1.0, name=\"v1\")  # pinned to /job:ps/task:0 (defaults to /cpu:0)\n",
    "    v2 = tf.Variable(2.0, name=\"v2\")  # pinned to /job:ps/task:1 (defaults to /cpu:0)\n",
    "    v3 = tf.Variable(3.0, name=\"v3\")  # pinned to /job:ps/task:0 (defaults to /cpu:0)\n",
    "    s = v1 + v2            # pinned to /job:worker (defaults to task:0/cpu:0)\n",
    "    with tf.device(\"/task:1\"):\n",
    "        p1 = 2 * s         # pinned to /job:worker/task:1 (defaults to /cpu:0)\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            p2 = 3 * s     # pinned to /job:worker/task:1/cpu:0\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "\n",
    "with tf.Session(\"grpc://127.0.0.1:2221\", config=config) as sess:\n",
    "    v1.initializer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more files to read\n",
      "[array([[  4.00000000e+00,   5.00000000e+00],\n",
      "       [  1.00000000e+00,   8.62997533e-19]], dtype=float32), array([1, 0], dtype=int32)]\n",
      "[array([[ 7.,  8.]], dtype=float32), array([0], dtype=int32)]\n",
      "No more training instances\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "test_csv = open(\"my_test.csv\", \"w\")\n",
    "test_csv.write(\"x1, x2 , target\\n\")\n",
    "test_csv.write(\"1.,    , 0\\n\")\n",
    "test_csv.write(\"4., 5. , 1\\n\")\n",
    "test_csv.write(\"7., 8. , 0\\n\")\n",
    "test_csv.close()\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "features = tf.stack([x1, x2])\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "enqueue_instance = instance_queue.enqueue([features, target])\n",
    "close_instance_queue = instance_queue.close()\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    try:\n",
    "        while True:\n",
    "            sess.run(enqueue_instance)\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"No more files to read\")\n",
    "    sess.run(close_instance_queue)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"No more training instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coord = tf.train.Coordinator()\n",
    "#threads = tf.train.start_queue_runners(coord=coord)\n",
    "#filename_queue = tf.train.string_input_producer([\"test.csv\"])\n",
    "#coord.request_stop()\n",
    "#coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queue runners and coordinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 7.,  8.],\n",
      "       [ 4.,  5.]], dtype=float32), array([0, 1], dtype=int32)]\n",
      "[array([[  1.00000000e+00,   8.62997533e-19]], dtype=float32), array([0], dtype=int32)]\n",
      "No more training instances\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "features = tf.stack([x1, x2])\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "enqueue_instance = instance_queue.enqueue([features, target])\n",
    "close_instance_queue = instance_queue.close()\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "n_threads = 5\n",
    "queue_runner = tf.train.QueueRunner(instance_queue, [enqueue_instance] * n_threads)\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"No more training instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  4.00000000e+00,   5.00000000e+00],\n",
      "       [  1.00000000e+00,   8.62997533e-19]], dtype=float32), array([1, 0], dtype=int32)]\n",
      "[array([[ 7.,  8.]], dtype=float32), array([0], dtype=int32)]\n",
      "No more training instances\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "def read_and_push_instance(filename_queue, instance_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "    features = tf.stack([x1, x2])\n",
    "    enqueue_instance = instance_queue.enqueue([features, target])\n",
    "    return enqueue_instance\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "    capacity=10, min_after_dequeue=2,\n",
    "    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "    name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "\n",
    "minibatch_instances, minibatch_targets = instance_queue.dequeue_up_to(2)\n",
    "\n",
    "read_and_enqueue_ops = [read_and_push_instance(filename_queue, instance_queue) for i in range(5)]\n",
    "queue_runner = tf.train.QueueRunner(instance_queue, read_and_enqueue_ops)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    sess.run(close_filename_queue)\n",
    "    coord = tf.train.Coordinator()\n",
    "    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run([minibatch_instances, minibatch_targets]))\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"No more training instances\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting a timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "6.0\n",
      "3.0\n",
      "4.0\n",
      "Timed out while dequeuing\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[()])\n",
    "v = tf.placeholder(tf.float32)\n",
    "enqueue = q.enqueue([v])\n",
    "dequeue = q.dequeue()\n",
    "output = dequeue + 1\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.operation_timeout_in_ms = 1000\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(enqueue, feed_dict={v: 1.0})\n",
    "    sess.run(enqueue, feed_dict={v: 2.0})\n",
    "    sess.run(enqueue, feed_dict={v: 3.0})\n",
    "    print(sess.run(output))\n",
    "    print(sess.run(output, feed_dict={dequeue: 5}))\n",
    "    print(sess.run(output))\n",
    "    print(sess.run(output))\n",
    "    try:\n",
    "        print(sess.run(output))\n",
    "    except tf.errors.DeadlineExceededError as ex:\n",
    "        print(\"Timed out while dequeuing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coming soon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
