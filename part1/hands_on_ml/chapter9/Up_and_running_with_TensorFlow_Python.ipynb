{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Hands-on 9장) Up and running with TensorFlow  (6/27, 일부분)\n",
    "\n",
    "### 텐서뽀개기, 이상열\n",
    "\n",
    "- 스터디 : 캐글뽀개기 - 텐서뽀개기\n",
    "- 페이스북 : <https://www.facebook.com/groups/kagglebreak/>\n",
    "- github : <https://github.com/KaggleBreak/tensorbreak>\n",
    "- 구글드라이브 : \n",
    "<https://drive.google.com/drive/folders/0B2l0iH28o85xM3A3TVhGdkFHb3M>\n",
    "\n",
    "\n",
    "### Intro\n",
    "- TensorFlow는 수치 계산을위한 강력한 오픈 소스 소프트웨어 라이브러리이며, 특히 대규모 기계 학습을 위해 매우 적합하고 미세 조정됩니다. 기본 원칙은 간단합니다. 먼저 파이썬에서 수행 할 계산 그래프 (예 : 그림 9-1의 그래프)를 정의한 다음 TensorFlow는 그래프를 가져 와서 최적화 된 C ++ 코드를 사용하여 효율적으로 실행합니다.\n",
    "\n",
    "<img src ='./img/chap9_1.png'>\n",
    "\n",
    "- 가장 중요한 점은 그래프를 여러 개의 청크로 분할하고 여러 개의 CPU 또는 GPU에서 병렬로 실행할 수 있다는 것입니다 (그림 9-2 참조). \n",
    "\n",
    "- TensorFlow는 분산 컴퓨팅을 지원하기 때문에 계산을 수백 대의 서버로 나누어 엄청난 양의 엄청난 양의 훈련을 엄청나게 훈련시킬 수 있습니다 (12 장 참조). TensorFlow는 수십억 개의 인스턴스와 수백만 개의 기능으로 구성된 교육 세트에서 수백만 개의 매개 변수가있는 네트워크를 교육 할 수 있습니다. TensorFlow가 존재했기 때문에 이것은 놀랄 일이 아닙니다.\n",
    "\n",
    "<img src ='./img/chap9_2.png'>\n",
    "\n",
    "- TensorFlow가 2015 년 11 월에 오픈 소스가되었을 때 이미 Deep Learning을위한 많은 오픈 소스 라이브러리가 있었고 (표 9-1은 몇 가지를 나열했습니다), TF의 기능 중 상당 부분은 이미 하나의 라이브러리 또는 다른 라이브러리에 존재했습니다. 그럼에도 불구하고 Tensor-Flow의 깔끔한 설계, 확장 성, 유연성 및 훌륭한 문서 (Google의 이름은 말할 것도 없음)가 신속하게 목록 상단으로 올라갔습니다. TF의 하이라이트 중 일부는 다음과 같습니다.\n",
    "\n",
    "    - Linux 및 MacOSX뿐만 아니라 iOS 및 Android를 포함한 모바일 장치에서도 실행됩니다.\n",
    "    - Scikit-Learn와 호환되는 TF.Learn 2 (tensorflow.con trib.learn)라는 매우 간단한 파이썬 API를 제공합니다. 사용자가 볼 수 있듯이이를 사용하여 교육 할 수 있습니다\n",
    "    - 또한 TF-slim (tensorflow.contrib.slim)이라는 간단한 API를 제공하여 신경망을 작성, 교육 및 평가하는 작업을 단순화합니다.\n",
    "    - Keras 또는 Pretty Tensor와 같은 Tensor-Flow 상단에 몇 가지 다른 고급 API가 독립적으로 구축되었습니다.\n",
    "    - 주요 파이썬 API는 생각할 수있는 신경망 아키텍처를 비롯하여 모든 종류의 계산을 생성 할 수있는 훨씬 더 많은 유연성 (높은 복잡성으로 인해)을 제공합니다.\n",
    "    - 특히 ML 네트워크를 구축하는 데 필요한 많은 ML 작업에 대해 매우 효율적인 C ++ 구현을 포함합니다. 또한 자신의 고성능 조작을 정의하는 C ++ API가 있습니다.\n",
    "    - 비용 기능을 최소화하는 매개 변수를 검색 할 수 있는 몇 가지 고급 최적화 노드를 제공합니다. TensorFlow는 사용자가 정의한 함수의 그라디언트를 자동으로 계산하기 때문에 사용하기가 매우 쉽습니다. 이를 자동 분산 (autodi)이라고합니다.\n",
    "    - 또한 TensorBoard라는 훌륭한 시각화 도구가 있어 계산 그래프를 탐색하고 학습 곡선을 보는 등의 작업을 수행 할 수 있습니다.\n",
    "    - Google은 TF 그래프 (https://cloud.google.com/ ml /)를 실행하기 위해 클라우드 서비스도 시작했습니다.\n",
    "\n",
    "<img src ='./img/chap9_3.png'>\n",
    "\n",
    "\n",
    "### Installation (생략)\n",
    "\n",
    "### Creating your first graph and running it in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "\n",
    "f=x*x*y+y+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'x_2:0' shape=() dtype=int32_ref>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'y_2:0' shape=() dtype=int32_ref>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_5:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이해해야 할 가장 중요한 점은이 코드가 실제로는 (특히 마지막 줄) 보이더라도 실제로는 계산을 수행하지 않는다는 것입니다. \n",
    "- 단지 계산 그래프를 생성합니다. 실제로 변수조차도 아직 초기화되지 않았습니다. \n",
    "- 이 그래프를 평가하려면 TensorFlow 세션을 열고이를 사용하여 변수를 초기화하고 f를 평가해야합니다. \n",
    "- TensorFlow 세션은 CPU 및 GPU와 같은 장치에 작업을 배치하고 실행하며 모든 변수 값을 보유합니다. 다음 코드는 세션을 생성하고, 변수를 초기화하고, f를 평가 한 다음 세션을 닫음으로써 자원을 확보합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(x.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(y.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = sess.run(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess: \n",
    "    x.initializer.run() \n",
    "    y.initializer.run() \n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with 블록의 세션은 기본 세션으로 설정됩니다. \n",
    "- x.initial izer.run()을 호출하는 것은 tf.get_default_session().run (x.initial izer)를 호출하는 것과 동일하며, 마찬가지로 f.eval ()은 tf.get_default_session().run (f)를 호출하는 것과 동일합니다. 이렇게하면 코드를 읽기 쉽게 만듭니다. 또한 세션이 블록 끝에서 자동으로 닫힙니다.\n",
    "- 모든 단일 변수에 대해 이니셜 라이저를 수동으로 실행하는 대신 initialize_all_variable () 함수를 사용할 수 있습니다. 실제로 초기화를 실제로 수행하지는 않으며 그래프가 실행될 때 모든 변수를 초기화 할 노드를 그래프로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables \n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jupyter 내부 또는 Python 셸 내에서 Interactive Sesion을 만드는 것이 더 좋습니다. \n",
    "- 일반 세션과의 유일한 차이점은 자동 세션이 생성되면 자동으로 기본 세션으로 설정되므로 with 블록이 필요하지 않습니다 (세션을 완료 할 때 세션을 수동으로 닫아야 함) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorFlow 프로그램은 일반적으로 두 부분으로 나뉩니다. \n",
    "- 첫 번째 부분은 계산 그래프를 작성하고 (두 번째 단계는 실행 단계라고 함) \n",
    "- 두 번째 부분은 실행 단계입니다. \n",
    "- 구축 단계에서는 일반적으로 ML 모델과이를 계발하는 데 필요한 계산을 나타내는 계산 그래프를 작성합니다. \n",
    "- 실행 단계는 일반적으로 반복적으로 교육 단계를 평가하는 루프를 실행합니다 (예 : 미니 배치 당 하나의 단계). 모델 매개 변수가 점진적으로 개선됩니다. 아래 예를 통해 살펴 보겠습니다.\n",
    "\n",
    "### Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = tf.Variable(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분의 경우 이 방법이 좋지만 때로는 여러 개의 독립적 인 그래프를 관리하고자 할 수 있습니다. \n",
    "- 새 그래프를 만들고 일시적으로 기본 그래프를 with 블록 안에 만들면 다음과 같이 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jupyter (또는 python 쉘)에서는 실험하는 동안 동일한 명령을 두 번 이상 실행하는 것이 일반적입니다. \n",
    "- 결과적으로 많은 중복 노드가 포함 된 기본 그래프로 끝날 수 있습니다. 한 가지 해결책은 Jupyter 커널 (또는 파이썬 쉘)을 다시 시작하는 것이지만 더 편리한 해결책은 tf.reset_default_graph ()를 실행하여 기본 그래프를 재설정하는 것입니다.\n",
    "\n",
    "\n",
    "### Lifecycle of a node value\n",
    "\n",
    "- 노드를 평가할 때 TensorFlow는 의존하는 노드 집합을 자동으로 결정하고이 노드를 먼저 평가합니다. 예를 들어, 다음 코드를 고려하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3) \n",
    "x=w+2 \n",
    "y=x+5 \n",
    "z=x*3\n",
    "with tf.Session() as sess: \n",
    "    print(y.eval()) # 10 \n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫째,이 코드는 매우 간단한 그래프를 정의합니다. \n",
    "- 다음 세션을 시작하고 그래프를 실행하여 y를 평가합니다. TensorFlow는 y가 w에 따라 달라 지므로 x에 의존하므로 자동으로 w, x, y를 평가하고 y 값을 반환합니다. 마지막으로 코드는 그래프를 실행하여 z를 평가합니다. 다시 한번, TensorFlow는 먼저 w와 x를 평가해야 함을 감지합니다. w와 x의 이전 평가 결과를 재사용하지 않는다는 점에 유의해야합니다. 간단히 말해 위의 코드는 w와 x를 두 번 평가합니다.\n",
    "- 모든 노드 값은 그래프 실행 사이에 세션에 의해 유지되는 변수 값을 제외하고는 그래프 실행 사이에서 삭제됩니다 (큐와 판독기는 12 장에서 보듯이 일부 상태를 유지함). 변수는 이니셜 라이저가 실행되면 수명이 시작되고 세션이 닫히면 끝납니다.\n",
    "- 위 코드에서와 같이 w와 x를 두 번 평가하지 않고 y와 z를 효율적으로 계산하려면 다음 코드와 같이 TensorFlow에게 하나의 그래프 실행에서 y와 z를 모두 평가하도록 요청해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z]) \n",
    "    print(y_val) # 10 \n",
    "    print(z_val) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단일 프로세스 TensorFlow에서 여러 세션은 같은 그래프를 재사용하더라도 각 세션마다 상태를 공유하지 않습니다 (각 세션에는 모든 변수의 자체 복사본이 있음). Distributed TensorFlow (제 12 장 참조)에서는 가변 상태가 세션이 아닌 서버에 저장되므로 여러 세션이 동일한 변수를 공유 할 수 있습니다.\n",
    "\n",
    "### Linear Regression with TensorFlow\n",
    "\n",
    "- TensorFlow 작업은 임의의 수의 입력을 가져 와서 여러 출력을 생성 할 수 있습니다. 예를 들어, 가산 연산은 각각 2 개의 입력을 취하여 1 개의 출력을 생성합니다. 상수와 변수는 입력이 없습니다 (소스 작업이라고 함). \n",
    "\n",
    "- 인풋과 아웃풋은 tenors라고 불리는 다차원 배열입니다. NumPy 배열과 마찬가지로 텐서는 유형과 모양을 가집니다. 실제로 파이썬 API에서 텐서는 단순히 NumPy ndarrays로 표현됩니다. 대개는 부동 소수점을 포함하지만 문자열 (임의의 바이트 배열)을 전달하는데도 사용할 수 있습니다.\n",
    "\n",
    "- 위의 예제에서, 텐서는 단지 하나의 스칼라 값을 포함하고 있지만 물론 어떤 모양의 배열에 대해서도 계산을 수행 할 수 있습니다. 예를 들어 다음 코드는 2 차원 배열을 조작하여 캘리포니아 주택 데이터 세트 (2 장에서 소개 함)에 선형 회귀를 수행합니다. 먼저 데이터 집합을 가져 와서 모든 학습 인스턴스에 추가 바이어스 입력 기능 (x0 = 1)을 추가합니다 (NumPy를 사용하여 수행되므로 즉시 실행 됨)\n",
    "\n",
    "- 이 데이터와 대상을 보유하는 두 개의 TensorFlow 상수 노드 X와 Y를 만들고 Tensor-Flow에서 제공하는 일부 매트릭스 작업을 사용하여 theta를 정의합니다. 이 행렬 함수 (transpose (), matmul () 및 matrix_inverse ())는 자명하지만, 평소와 같이 계산을 즉시 수행하지 않고 대신 그래프를 실행할 때 그래프를 수행하는 노드를 만듭니다 . theta의 정의는 다음과 일치한다는 것을 알 수 있습니다.\n",
    "\n",
    "- 정규 방정식 (θ = T · -1 · T ·, 4 장 참조). 마지막으로 코드는 세션을 만들고이를 사용하여 theta를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'California housing dataset.\\n\\nThe original database is available from StatLib\\n\\n    http://lib.stat.cmu.edu/\\n\\nThe data contains 20,640 observations on 9 variables.\\n\\nThis dataset contains the average house value as target variable\\nand the following input variables (features): average income,\\nhousing average age, average rooms, average bedrooms, population,\\naverage occupation, latitude, and longitude in that order.\\n\\nReferences\\n----------\\n\\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\nStatistics and Probability Letters, 33 (1997) 291-297.\\n\\n',\n",
       " 'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ..., \n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'target': array([ 4.526,  3.585,  3.521, ...,  0.923,  0.847,  0.894])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.        ,    8.3252    ,   41.        , ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   1.        ,    8.3014    ,   21.        , ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   1.        ,    7.2574    ,   52.        , ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ..., \n",
       "       [   1.        ,    1.7       ,   17.        , ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.        ,    1.8672    ,   18.        , ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   1.        ,    2.3886    ,   16.        , ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_plus_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess: \n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.74651413e+01],\n",
       "       [  4.35734153e-01],\n",
       "       [  9.33829229e-03],\n",
       "       [ -1.06622010e-01],\n",
       "       [  6.44106984e-01],\n",
       "       [ -4.25131839e-06],\n",
       "       [ -3.77322501e-03],\n",
       "       [ -4.26648885e-01],\n",
       "       [ -4.40514028e-01]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654266e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드가 NumPy를 사용하여 직접적으로 수식을 계산하는 것보다 TensorFlow가 자동으로 GPU 카드에서이 코드를 실행할 수 있다는 점이 이점입니다 (GPU를 지원하는 TensorFlow를 설치했다면 12 장 참조).\n",
    "\n",
    "### Implementing Gradient Descent\n",
    "\n",
    "- Normal Equations 대신 Batch Gradient Descent (4 장 참조)를 사용해 보겠습니다. 먼저 그라디언트를 수동으로 계산하여이를 수행 한 다음 TF가 자동으로 그라디언트를 계산할 수 있도록 TenorFlow의 자동 분산 기능을 사용하고 마지막으로 TessorFlow의 몇 가지 옵티 마이저를 사용합니다.\n",
    "\n",
    "### Manually computing the gradients\n",
    "- 다음 코드는 몇 가지 새로운 요소를 제외하고는 상당히 자명합니다.\n",
    "    - random_uniform () 함수는 그래프에 NumPy의 rand () 함수와 마찬가지로 모양과 값 범위가 주어진 임의의 값을 포함하는 텐서를 생성하는 노드를 만듭니다.\n",
    "    - assign () 함수는 변수에 새 값을 할당 할 노드를 만듭니다. 이 경우 Batch Gradient Descent 단계 θ(next step) = θ-η∇θMSE θ를 구현합니다.\n",
    "    - 메인 루프는 트레이닝 단계를 반복 실행하고 (n_epochs times) 100 회 반복 할 때마다 현재 Mean Squared Error (mse)를 인쇄합니다. 매 반복마다 MSE가 내려가는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.75443\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.57278\n",
      "Epoch 300 MSE = 0.558501\n",
      "Epoch 400 MSE = 0.549069\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.537379\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.531243\n",
      "Epoch 900 MSE = 0.529371\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using autodiff\n",
    "\n",
    "- 위의 코드는 정상적으로 작동하지만 비용 함수 (MSE)에서 그라디언트를 수학적으로 유도해야 합니다. 선형 회귀 분석의 경우에는 비교적 쉽지만,  deep neural networks를 사용하여이 작업을 수행해야 한다면 두통이 심해질 수 있습니다. 지루하고 오류가 발생하기 쉽습니다. 기호 편차를 사용하여 편미분 방정식의 방정식을 자동으로 찾을 수 있지만 결과 코드가 반드시 효율적이지는 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_func(a, b): \n",
    "    z=0\n",
    "    for i in range(100): \n",
    "        z=a*np.cos(z+i)+z*np.sin(a-i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다행스럽게도 TF의 autodiff 기능은 자동으로 효율적으로 그라디언트를 계산할 수 있습니다.\n",
    "- 이전 섹션의 Gradient Descent 코드의 gradients = ... 라인을 다음 라인으로 바꾸기 만하면 코드가 계속 잘 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23493649049060805"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradients () 함수는 (이 경우 mse)와 변수 목록 (이 경우에는 theta)을 취하고 각 변수와 관련하여 그래디언트를 계산하기 위해 변수 당 하나의 목록을 만듭니다. \n",
    "- 그래디언트 노드는 쎄타와 관련하여 MSE의 그래디언트 벡터를 계산합니다.\n",
    "- 주로 그라데이션을 계산하는 4 가지 방법이 있습니다. 그것들은 표 9-2에 요약되어있다. \n",
    "- TensorFlow는 역 입력 자동 모드를 사용합니다.이 모드는 입력이 많고 출력이 적을 때 완벽하고 (효율적이고 정확합니다), 종종 신경 네트워크 에서처럼 그러합니다. noutputs + 1 그래프 횡단에서 모든 입력에 대한 출력의 모든 부분 파생 값을 계산합니다.\n",
    "\n",
    "\n",
    "### Using an optimizer\n",
    "- 따라서 TensorFlow는 사용자의 그라디언트를 계산합니다. TensorFlow는 그래디언트 하강 옵티 마이저 (Gradient Descent Opti-izer)를 포함하여 다양한 옵티 마이저를 즉시 제공합니다. 위의 그래디언트 = ... 및 training_op = ... 행을 다음 코드로 대체하면 모든 것이 올바르게 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.75443\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.57278\n",
      "Epoch 300 MSE = 0.558501\n",
      "Epoch 400 MSE = 0.549069\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.537379\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.531243\n",
      "Epoch 900 MSE = 0.529371\n",
      "Best theta:\n",
      "[[  2.06855249e+00]\n",
      " [  7.74078071e-01]\n",
      " [  1.31192386e-01]\n",
      " [ -1.17845066e-01]\n",
      " [  1.64778143e-01]\n",
      " [  7.44078017e-04]\n",
      " [ -3.91945094e-02]\n",
      " [ -8.61356676e-01]\n",
      " [ -8.23479772e-01]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding data to the training algorithm\n",
    "\n",
    "- 위의 코드를 수정하여 Mini-batch Gradient Descent를 구현해 보겠습니다. 이를 위해서는 모든 반복마다 X와 Y를 다음 미니 배치로 바꾸는 방법이 필요합니다. 가장 간단한 방법은 자리 표시 자 노드를 사용하는 것입니다. 이 노드는 실제로 계산을 수행하지 않기 때문에 특별합니다. 런타임시 출력하도록 지정한 데이터 만 출력합니다. 런타임에 자리 표시 자에 값을 지정하지 않으면 예외가 발생합니다.\n",
    "\n",
    "- 자리 표시 자 노드를 만들려면 placeholder() 함수를 호출하고 출력 텐서의 데이터 형식을 지정해야합니다. 선택적으로 적용하려는 경우 모양을 지정할 수도 있습니다. 차원에 없음을 지정하면 \"모든 크기\"를 의미합니다. 예를 들어, 다음 코드는 자리 표시 자 노드 A와 노드 B = A + 5를 만듭니다. B를 평가할 때 A 값을 지정하는 eval () 메서드에 feed_dict를 전달합니다. A는 rank 2 (즉 2 차원이어야 함)이며 3 개의 열이 있어야합니다 (그렇지 않으면 예외가 발생 함). 그러나 행 수에는 제한이 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mini-batch Gradient Descent를 구현하려면 기존 코드를 약간만 조정하면됩니다. 먼저 생성 단계에서 X와 Y의 정의를 변경하여 자리 표시 자 노드로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런 다음 배치 크기를 정의하고 총 배치 수를 계산하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막으로 실행 단계에서 하나씩 미니 배치를 가져온 다음 둘 중 하나에 종속 된 노드를 평가할 때 feed_dict 매개 변수를 통해 X 및 y 값을 제공하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07001591],\n",
       "       [ 0.82045609],\n",
       "       [ 0.1173173 ],\n",
       "       [-0.22739051],\n",
       "       [ 0.31134021],\n",
       "       [ 0.00353193],\n",
       "       [-0.01126994],\n",
       "       [-0.91643935],\n",
       "       [-0.87950081]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and restoring models\n",
    "- 모델을 교육 한 후에는 매개 변수를 디스크에 저장해야 원하는 때 언제든지 다시 돌아와 다른 프로그램에서 사용하고 다른 모델과 비교하는 등의 작업을 수행 할 수 있습니다. 또한 트레이닝 중에 정기적 인 체크 포인트를 저장하기를 원할 것입니다. 따라서 트레이닝 중에 컴퓨터가 다운되는 경우 처음부터 다시 시작하지 않고 마지막 체크 포인트부터 계속할 수 있습니다.\n",
    "\n",
    "- TensorFlow를 사용하면 모델을 쉽게 저장하고 복원 할 수 있습니다. 모든 변수 노드가 생성 된 후 생성 단계의 끝에서 Saver 노드를 생성 한 다음 실행 단계에서 모델을 저장하고 체크 포인트의 세션과 경로를 전달할 때마다 save () 메서드를 호출하기 만하면됩니다. \n",
    "\n",
    "- 모델을 복원하는 것 역시 쉽습니다. 위와 같이 생성 단계가 끝날 때 Saver를 만들었지 만 실행 단계가 시작될 때 init 노드를 사용하여 변수를 초기화하는 대신 restore () 메서드를 호출합니다 보호기 개체 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.75443\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.57278\n",
      "Epoch 300 MSE = 0.558501\n",
      "Epoch 400 MSE = 0.549069\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.537379\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.531243\n",
      "Epoch 900 MSE = 0.529371\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                # not shown\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적으로 Saver는 모든 변수를 자체 이름으로 저장 및 복원하지만 더 많은 제어가 필요한 경우 저장 또는 복원 할 변수와 사용할 이름을 지정할 수 있습니다. 예를 들어 다음 Saver는 이름 가중치 아래에 theta 변수 만 저장하거나 복원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the graph and training curves using Tensorboard\n",
    "\n",
    "- 이제 Mini-batch Gradient Descent를 사용하여 선형 회귀 모델을 학습하는 계산 그래프를 얻었으며 일정 간격으로 검사 점을 저장합니다. \n",
    "\n",
    "- 정교한 것처럼 들리니? 그러나 우리는 여전히 print () 함수에 의존하여 훈련 중 진행 상황을 시각화합니다. 더 좋은 방법이 있습니다 : \n",
    "\n",
    "- TensorBoard를 입력하십시오. 교육 통계를 제공하면 웹 브라우저에서 이러한 통계를 대화 형으로 시각적으로 표시 할 수 있습니다 (예 : 학습 곡선). 또한 그래프의 정의를 제공 할 수 있으며이를 통해 탐색 할 수있는 훌륭한 인터페이스를 제공합니다. 이것은 그래프의 오류를 식별하고 병목 현상을 찾는 등 매우 유용합니다.\n",
    "\n",
    "- 첫 번째 단계는 프로그램을 약간 조정하여 그래프 정의와 일부 교육 통계 (예 : 교육 오류 (MSE))를 TenorBoard에서 읽을 로그 디렉터리에 기록하는 것입니다. 프로그램을 실행할 때마다 다른 로그 디렉토리를 사용해야합니다. 그렇지 않으면 TensorBoard가 다른 실행의 통계를 병합하여 시각화를 엉망으로 만듭니다. 가장 간단한 해결책은 로그 디렉토리 이름에 시간 소인을 포함시키는 것입니다. 프로그램 시작 부분에 다음 코드를 추가하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫 번째 줄은 MSE 값을 평가하고 요약이라는 TensorBoard 호환 바이너리 로그 문자열에 쓰는 노드를 그래프에 만듭니다. \n",
    "- 두 번째 줄은 요약 파일을 로그 디렉토리에 기록하는 데 사용하는 SummaryWriter를 만듭니다. \n",
    "- 첫 번째 매개 변수는 로그 디렉토리의 경로를 나타냅니다 (이 경우 현재 디렉토리와 관련하여 tf_logs / run-20160906091959 /와 같은 것). 두 번째 매개 변수 (선택 사항)는 시각화하려는 그래프입니다. \n",
    "- 생성시, SummaryWriter는 로그 디렉토리가 아직 존재하지 않으면 (그리고 필요한 경우 상위 디렉토리) 로그 파일을 작성하고 그래프 파일 정의를 이벤트 파일이라는 2 진 로그 파일에 작성합니다.\n",
    "- 다음으로 트레이닝 중에 mse_summary 노드를 정기적으로 평가하기 위해 실행 단계를 업데이트해야합니다 (예 : 10 개의 미니 배치마다). summary_writer를 사용하여 이벤트 파일에 쓸 수있는 요약을 출력합니다. 다음은 업데이트 된 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
