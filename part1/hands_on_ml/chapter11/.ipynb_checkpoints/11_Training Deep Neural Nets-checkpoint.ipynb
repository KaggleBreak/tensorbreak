{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서뽀개기 Part1  2017/08/08(화) 4회차 모임\n",
    "### Hands on Machine learning scikit_learn_and_Tensorflow\n",
    "\n",
    "- 스터디 : 캐글뽀개기 - 텐서뽀개기\n",
    "- 페이스북 : <https://www.facebook.com/groups/kagglebreak/>\n",
    "- github : <https://github.com/KaggleBreak/tensorbreak>\n",
    "- 구글드라이브 : \n",
    "<https://drive.google.com/drive/folders/0B2l0iH28o85xM3A3TVhGdkFHb3M>\n",
    "\n",
    "\n",
    "### 11장. Training Deep Neural Nets\n",
    "\n",
    "- 10 장에서는 인공 신경망을 소개하고 우리의 첫 번째 딥 신경망 (Deep Neural Network)을 훈련했습니다. \n",
    "- 그러나 이것은 매우 얕은 DNN이었으며 두 개의 숨겨진 레이어만 있었습니다. \n",
    "- 고해상도 이미지에서 수백 가지 유형의 객체를 감지하는 것과 같이 매우 복잡한 문제를 해결해야하는 경우에는 어떻게 해야 합니까? \n",
    "- 훨씬 깊이있는 DNN을 훈련해야 할 수도 있습니다. 예를 들어 수백 개의 뉴런을 포함하는 10 개의 레이어가 수천 개의 연결 고리로 연결되어 있습니다. \n",
    "\n",
    "\n",
    "    1. 첫째, Deep Neural Nets에 영향을 미치고 하위 계층을 매우 힘들게 훈련시키는 까다로운 그라디언트 손실 문제 (또는 그라디언트 폭발 문제)에 직면하게됩니다.\n",
    "\n",
    "    2. 둘째, 대규모 네트워크에서 훈련은 매우 느립니다.\n",
    "\n",
    "    3. 셋째, 수백만 개의 매개 변수가 있는 모델은 훈련 세트에 과도한 영향을 줄 수 있습니다.\n",
    "\n",
    "- 이 장에서는 이러한 각 문제를 차례대로 살펴보고 해결 방법을 제시합니다. 우리는 사라지는 그라디언트 문제를 설명하고 이 문제에 대한 가장 대중적인 해결책을 모색하는 것으로 시작할 것입니다. \n",
    "\n",
    "- 다음으로 우리는 다양한 그라디언트 디센트와 비교하여 대용량 모델을 엄청나게 빠르게 훈련할 수있는 다양한 옵티마이저를 살펴볼 것입니다.\n",
    "- 마지막으로 대규모 신경망에 대한 몇 가지 인기있는 정규화 기술을 살펴 보겠습니다. \n",
    "\n",
    "\n",
    "### Vanishing/exploding gradients problems\n",
    "- 역전파 알고리즘은 출력 레이어에서 입력 레이어로 이동하여 그라디언트의 에러를 전파합니다. 알고리즘이 네트워크의 각 매개 변수와 관련하여 비용 함수의 그래디언트를 계산하여 사용하면 각 매개 변수를 Gradient Descent 단계로 업데이트 합니다.\n",
    "\n",
    "- 불행하게도, 그라디언트는 알고리즘이 하위 레이어로 진행함에 따라 종종 작아지고 작아집니다. 결과적으로 Gradient Descent 업데이트는 하위 계층 연결 가중치를 거의 변경하지 않으며 훈련은 좋은 솔루션으로 수렴하지 않습니다. 이것을 사라지는 그라디언트 문제라고 합니다. 경우에 따라 그 반대가 발생할 수 있습니다. 즉, 그래디언트가 커질 수 있기 때문에 하위 레이어가 대폭 업데이트 되고 알고리즘이 분기됩니다. 이것은 폭발하는 그라디언트 문제입니다. \n",
    "\n",
    "- 보다 일반적으로, deep neural network는 불안정한 그라디언트로 인해 어려움을 겪습니다. 즉, 서로 다른 계층이 서로 다른 속도로 학습 할 수 있습니다.\n",
    "\n",
    "    - 이러한 불행한 행동이 경험적으로 관찰되었지만 (깊은 신경망이 대부분 오랫동안 버려진 이유 중 하나이기도 합니다), 2010 년 전후로는 이를 이해하는 데 상당한 진전이 있었습니다. \n",
    "\n",
    "- Xavier Glorot와 Y. Bengio의 \"깊은 피드 포워드 신경 네트워크 훈련의 어려움을 이해하는 것\"은 인기있는 시그모이드 활성화 함수와 그 당시 가장 대중적이었던 weight 초기화 기술의 조합을 포함하여 몇 가지 해결책을 발견했습니다. 즉, 평균 0과 표준 편차 1의 정규 분포를 사용하는 무작위 초기화입니다. \n",
    "\n",
    "    - 간단히 말해서,이 활성화 함수와 초기화 방법을 사용하면 각 계층의 출력의 편차가 그 입력. 네트워크에서 앞으로 나아가면, 활성화 함수가 최상위 레이어에서 포화될 때까지 각 레이어 이후에 분산이 계속 증가합니다. 이것은 실제로 로지스틱 함수가 0.5가 아니라 0이 아니라는 사실에 의해 악화됩니다 (쌍곡선 탄젠트 함수는 평균이 0이고 깊은 네트워크에서 로지스틱 함수보다 약간 더 나은 행동을 보입니다).\n",
    "\n",
    "- 로지스틱 활성화 함수 (그림 11-1 참조)를 보면 입력이 커지면 (음수 또는 양수) 함수가 0 또는 1로 포화하고 미분이 0에 가깝다는 것을 알 수 있습니다. 따라서 역전파가 시작될 때, 네트워크를 통해 다시 확산되는 그라디언트가 거의 없으며, 최상위 레이어를 통해 역전파가 진행됨에 따라 약간의 그라디언트가 희박해져서 하위 레이어에는 아무것도 남지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAF3CAYAAAAfLsn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntxs3a/oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121a4b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He initialization\n",
    "\n",
    "- Glorot과 Bengio는 논문에서 문제를 상당히 완화시키는 방법을 제안합니다. \n",
    "\n",
    "- 우리는 신호가 양방향으로 적절히 흐를 필요가 있습니다: 예측을 할 때 앞 방향으로, 그래디언트를 역전파 할 때 반대 방향으로 신호가 필요합니다. \n",
    "\n",
    "- 각 층의 출력의 분산이 입력의 분산과 동일해야한다고 주장하고 반대 방향의 층을 통과하기 전후의 균등 분산을 필요로 한다고 주장한다 (수학적 세부 사항에 관심이 있다면이 논문을 확인하십시오).\n",
    "\n",
    "- 레이어가 동일한 수의 입출력 연결을 갖고 있지 않으면 실제로 보장 할 수는 없지만 실제로 잘 작동하는 것으로 입증된 좋은 절충안을 제안했습니다. \n",
    "\n",
    "- 연결 가중치는 식 11에 설명된대로 무작위로 초기화해야합니다. 여기서 n 개의 입력 및 n 개의 출력은 가중치가 초기화되는 계층 (팬 - 인 및 팬 - 아웃이라고도 함)에 대한 입력 및 출력 연결 수입니다. \n",
    "\n",
    "- 이 초기화 전략은 Xavier 초기화 (작성자의 이름 뒤에) 또는 Glorot 초기화라고도 합니다.\n",
    "\n",
    "![img11_1](img/ch11_2.png)\n",
    "\n",
    "- 입력 연결 수가 출력 연결 수와 대략 같으면 간단한 방정식을 얻을 수 있습니다. 이 간단한 전략을 10 장에서 사용했습니다. (eg. σ = 1/ root(n inputs) or r = 3/ root(n inputs) \n",
    "\n",
    "- Xavier 초기화 전략을 사용하면 훈련 속도가 상당히 빨라질 수 있으며 Deep Learning의 현재 성공을 이끌어 낸 트릭 중 하나입니다. 표 11-1에서 볼 수 있듯이 최근의 일부 논문 3은 다양한 활성화 기능에 대해 유사한 전략을 제공했습니다. ReLU 활성화 함수 (및 아래에 설명 된 ELU 활성화를 비롯한 다양한 변형)에 대한 초기화 전략은 때때로 He 초기화라고도합니다 (작성자의 성과 같음)\n",
    "\n",
    "![img11_1](img/ch11_3.png)\n",
    "\n",
    "- 기본적으로 fully_connected() 함수 (제 10 장에서 소개)는 Xavier 초기화 (균일 분포)를 사용합니다. variance_scaling_initializer() 함수를 사용하여 이를 He 초기화로 변경할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의 :이 책은 `tf.layers.dense ()`(이 장을 쓸 때 없었던) 대신 `tensorflow.contrib.layers.fully_connected()`를 사용합니다. contrib 모듈의 내용은 예고없이 변경되거나 삭제 될 수 있으므로 `tf.layers.dense()`를 사용하는 것이 더 바람직합니다. `dense()`함수는 `fully_connected ()` 함수와 거의 같습니다. 이 장과 관련된 주요 차이점은 다음과 같습니다.\n",
    "\n",
    "* 여러 매개 변수의 이름이 바뀌 었습니다 :`scope`는 `name`이 되고,`activation_fn`은 `activation`이 됩니다 (마찬가지로 `_fn` 접미사는`normalizer_fn` 같은 다른 매개 변수에서 제거됩니다),`weights_initializer`는 `kernel_initializer` 등이 됩니다. \n",
    "* 디폴트 `activation`은 `tf.nn.relu`보다는 `None`입니다.\n",
    "*`tensorflow.contrib.framework.arg_scope()`를 지원하지 않습니다 (11 장에서 나중에 소개됩니다).\n",
    "* 정규 표현식 매개 변수는 지원하지 않습니다 (11 장에서 나중에 소개)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.contrib.layers.python.layers.initializers.variance_scaling_initializer.<locals>._initializer>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/Relu:0' shape=(?, 300) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-saturating activation functions (비포화 활성화 함수)\n",
    "\n",
    "- X. Glorot와 Y. Bengio의 2010년 논문에서 얻은 통찰력 중 하나는 부분적으로는 활성화 기능의 선택이 좋지 않아서 사라지는 / 폭발하는 그라디언트 문제라는 것입니다. \n",
    "\n",
    "- 그때까지 대부분의 사람들은 대자연이 생물학적 뉴런에서 대략적인 시그모이드 활성화 기능을 사용하기로 결정했다면 그것은 탁월한 선택이어야 한다고 생각했습니다. 그러나 다른 활성화 함수는 깊은 신경망, 특히 ReLU 활성화 함수에서 훨씬 더 잘 작동합니다. 이는 주로 양수 값에 대해 포화되지 않기 때문에 (또한 계산 속도가 빠르기 때문입니다).\n",
    "\n",
    "- 불행하게도, ReLU 활성화 기능은 완벽하지 않습니다. 그것은 죽어가는 ReLUs로 알려진 문제로 고통받고 있습니다 : 훈련 중, 일부 뉴런은 효과적으로 죽습니다. 즉, 0 이외의 출력을 멈추는 것을 의미합니다. 어떤 경우에는 네트워크의 뉴런 중 절반이 죽어있을 수 있습니다. 특히 큰 학습 속도. 훈련 도중 뉴런의 가중치가 음수가 되는 방식으로 뉴런의 가중치가 업데이트되면 0이 출력됩니다.\n",
    "\n",
    "- 이것이 일어날 때, 입력이 음수 일 때 ReLU 함수의 그래디언트가 0이기 때문에 뉴런은 다시 생기지 않을 것입니다.\n",
    "\n",
    "- 이 문제를 해결하려면 Leaky ReLU와 같은 ReLU 함수의 변형을 사용하는 것이 좋습니다. \n",
    "\n",
    "- 이 함수는 Leaky ReLUα z = max (αz, z)로 정의됩니다 (그림 11-2 참조). \n",
    "\n",
    "- 하이퍼 파라미터 α는 함수가 \"Leaky\"정도를 정의합니다. 0보다 작을 때의 함수의 기울기입니다. 일반적으로 0.01로 설정됩니다. 작은 경사면은 Leaky ReLUs가 결코 죽지 않는다는 것을 보증합니다. 그들은 긴 혼수 상태에 빠질 수는 있지만 결국 깨어날 기회가 있습니다. 최근의 논문에서는 ReLU 활성화 함수의 여러 변종과 그 결론 중 하나를 비교 한 결과, Leaky 변이가 엄격한 ReLU 활성화 함수보다 항상 뛰어나다는 결론을 얻었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAF3CAYAAAC8MNLCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCJU1EVERCLi/wHf72+m/hx4XAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1045c0ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.951\n",
      "10 Batch accuracy: 0.96 Validation accuracy: 0.9666\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9722\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9748\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9768\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.9778\n",
      "35 Batch accuracy: 0.96 Validation accuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막으로 Djork-Arné Clevert 외 2015년 모든 ReLU 변형을 능가하는 Exponential Linear Unit (ELU) 라 불리는 새로운 활성화 함수를 제안했다. 훈련 시간이 단축되고 신경망이 테스트 세트에서 더 잘 수행되었다. 그림 11-3에 표현되어 있으며, 식 11-2는 그 정의를 보여줍니다.\n",
    "\n",
    "![img11_1](img/ch11_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 몇 가지 큰 차이점을 제외하고는 ReLU 기능과 매우 비슷합니다.\n",
    "- 먼저 z가 0보다 작을 때 음수 값을 취합니다. 이 값을 사용하면 장치의 평균 출력이 0에 가깝습니다. 위와 같이 소실점 그라디언트 문제를 완화하는 데 도움이 됩니다. 하이퍼 파라미터 α는 z가 큰 음수 일 때 ELU 함수가 접근하는 값을 정의합니다. 일반적으로 1로 설정되지만 원하는 경우 다른 하이퍼 매개 변수처럼 조정할 수 있습니다.\n",
    "\n",
    "- 둘째, z가 0보다 작을 때 0이 아닌 그라디언트가 있으므로 유닛이 죽어버리는 이슈가 발생하지 않습니다.\n",
    "- 셋째, 함수는 z = 0 주변을 포함하여 모든 면에서 매끄럽습니다. 그래디언트 하강 속도를 높이는 데 도움이 됩니다. \n",
    "- ELU 활성화 함수의 주된 단점은 ReLU 및 그 변형 (지수 함수 사용으로 인해)보다 느리게 계산되지만, 훈련 중에는 더 빠른 수렴 속도로 보상됩니다. 그러나 테스트 시간에 ELU 네트워크는 ReLU 네트워크보다 느립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAF5CAYAAAACiiltAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1161c5080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면 어떤 활성화 기능을 딥 뉴럴 네트워크에 사용해야 합니까? 귀하의 마일리지는 일반적으로 ELU > Leaky ReLU (및 그 변형) > ReLU > tanh > logistic에서 다양합니다. 런타임 성능이 중요하다면 ELU보다 Leaky ReLUs를 선호할 것입니다. 또 다른 하이퍼 파라미터를 조정하고 싶지 않다면 위에서 제안한 디폴트 α 값을 사용할 수 있습니다 (Leaky ReLU의 경우 0.01, ELU의 경우 1). \n",
    "\n",
    "- 여유 시간과 계산 능력이 있다면 상호 인증을 사용하여 다른 활성화 기능, 특히 네트워크가 과도한 경우 RReLU를, 또는 거대한 교육 세트가있는 경우 PReLU를 평가할 수 있습니다.\n",
    "\n",
    "\n",
    "### SELU\n",
    "\n",
    "- This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017 (I will definitely add it to the book). It outperforms the other activation functions very significantly for deep neural networks, so you should really try it out.\n",
    "\n",
    "- “scaled exponential linear units”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAF3CAYAAAA4gEgdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZFoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11570ef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n",
      "Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n",
      "Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n",
      "Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n",
      "Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n",
      "Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n",
      "Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n",
      "Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's a TensorFlow implementation (there will almost certainly be a tf.nn.selu() function in future TensorFlow versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SELUs can also be combined with dropout, check out [this implementation](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) by the Institute of Bioinformatics, Johannes Kepler University Linz.\n",
    "\n",
    "- Let's create a neural net for MNIST using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.96 Validation accuracy: 0.924\n",
      "5 Batch accuracy: 1.0 Validation accuracy: 0.957\n",
      "10 Batch accuracy: 0.94 Validation accuracy: 0.967\n",
      "15 Batch accuracy: 0.98 Validation accuracy: 0.9682\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9708\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.969\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9698\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "means = mnist.train.images.mean(axis=0, keepdims=True)\n",
    "stds = mnist.train.images.std(axis=0, keepdims=True) + 1e-10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            X_val_scaled = (mnist.validation.images - means) / stds\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_val_scaled, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- ELU (또는 ReLU의 변형)와 함께 초기화를 사용하면 훈련 초기에 사라지는 / 폭발하는 그라디언트 문제가 크게 줄어들지만 훈련 도중 돌아오지 않을 것이라고 보장하지는 않습니다.\n",
    "\n",
    "- 2015 년 논문 6에서 Sergey Ioffe와 Christian Szegedy는 소실 / 폭발 그라디언트 문제를 다루기 위해 Batch Normalization(BN)이라고 하는 기술을 제안했으며, 보다 일반적으로 이전 계층의 매개 변수로서 각 계층 입력의 분포가 변경되는 문제 레이어가 변경됩니다 (내부 공변량 문제)\n",
    "\n",
    "- 이 기법은 각 레이어의 활성화 기능 직전에 모델에 연산을 추가하고, 입력을 제로 - 센터링 및 정규화 한 다음 레이어 당 두 개의 새로운 매개 변수를 사용하여 결과를 스케일링하고 이동하는 것으로 구성됩니다 (하나는 스케일링 용이고 다른 하나는 시프트 용). 즉,이 작업을 통해 모델은 각 레이어의 입력 값의 최적 스케일 및 평균을 학습 할 수 있습니다.\n",
    "\n",
    "- 입력을 중심에 맞추고 정규화하기 위해 알고리즘은 입력의 평균 및 표준 편차를 추정해야 합니다. 그것은 현재 미니 배치 (따라서 \"Batch Normalization\"이라는 이름)를 통해 입력의 평균 및 표준 편차를 평가함으로써 그렇게 합니다. 전체 동작은 식 11-3에 요약되어 있습니다.\n",
    "\n",
    "![img11_1](img/ch11_5.png)\n",
    "\n",
    "- 테스트에는 경험적 평균과 표준 편차를 계산할 미니 배치가 없으므로 대신 전체 훈련 세트의 평균 및 표준 편차를 사용하십시오. \n",
    "\n",
    "- 이들은 일반적으로 이동 평균을 사용하여 훈련 중에 효율적으로 계산됩니다. 따라서 전체적으로 γ (스케일), β (오프셋), μ (평균) 및 σ (표준 편차)와 같은 각 배치 정규화 된 레이어에 대해 4 개의 매개 변수가 학습됩니다.\n",
    "\n",
    "- 이 기술이 실험 한 모든 딥 뉴럴 네트워크를 상당히 개선 시켰음을 보여주었습니다. 사라지는 그라디언트 문제는 tanh 및 로지스틱 활성화 함수와 같은 포화 활성화 함수를 사용할 수 있을 정도로 크게 감소했습니다. 네트워크는 또한 가중치 초기화에 덜 민감했습니다.\n",
    "\n",
    "- 훨씬 더 큰 학습 속도를 사용할 수 있었고, 학습 과정의 속도가 크게 빨라졌습니다. 일괄 정규화 된 네트워크의 앙상블을 바탕으로 ImageNet 분류에 대한 가장 잘 알려진 결과를 향상시킵니다 : 4.9 %의 상위 5 가지 유효성 검증 오류 (및 4.8 %의 테스트 오류)에 도달하여 사람 평가자의 정확도를 초과합니다. \n",
    "\n",
    "- \"마지막으로, Batch Normalization은 규칙 화기처럼 작동하여 다른 정규화 기술(예 : Dropout)을 사용할 필요가 없습니다.\n",
    "\n",
    "- 그러나 Batch Normalization은 모델에 약간의 복잡성을 추가합니다 (입력 데이터를 정규화 할 필요가 없지만 일차 숨겨진 레이어는 일괄 정규화를 제공하므로이를 처리 할 것이므로). 또한, 런타임 패널티가 있습니다. 신경망은 각 계층에 필요한 추가 계산으로 인해 예측 속도가 느려집니다. 따라서 예측이 번개처럼 빨라지려면 일괄 정규화로 재생하기 전에 ELU + He 초기화가 얼마나 잘 수행되는지 확인해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization with TensorFlow\n",
    "\n",
    "- TensorFlow는 입력을 센터링하고 정규화하는 batch_normalization() 함수를 제공하지만 사용자는 평균 및 표준 편차를 직접 계산해야합니다 (위에서 설명한 것처럼 트레이닝 중 미니 배치 데이터 또는 테스트 중 전체 데이터 세트를 기반으로 함) \n",
    " \n",
    "- 매개 변수로 이 함수에 전달하고 스케일링 및 오프셋 매개 변수의 생성을 처리해야합니다 (그리고 이 함수에 전달). 그것은 가능하지만 가장 편리한 접근법은 아닙니다. 대신이 모든 것을 처리하는 batch_norm() 함수를 사용해야합니다. 직접 호출하거나 fully_connected() 함수에 다음 코드와 같이 사용하도록 지시 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To avoid repeating the same parameters over and over again, we can use Python's partial() function:\n",
    "- [python Partial 설명](http://www.incodom.kr/%ED%8C%8C%EC%9D%B4%EC%8D%AC/%ED%95%A8%EC%88%98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function batch_normalization at 0x110a88488>, momentum=0.9, training=<tf.Tensor 'training:0' shape=() dtype=bool>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_batch_norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: since we are using tf.layers.batch_normalization() rather than tf.contrib.layers.batch_norm() (as in the book), we need to explicitly run the extra update operations needed by batch normalization (sess.run([training_op, extra_update_ops],...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8721\n",
      "1 Test accuracy: 0.8981\n",
      "2 Test accuracy: 0.9134\n",
      "3 Test accuracy: 0.9238\n",
      "4 Test accuracy: 0.9297\n",
      "5 Test accuracy: 0.9352\n",
      "6 Test accuracy: 0.9405\n",
      "7 Test accuracy: 0.9436\n",
      "8 Test accuracy: 0.947\n",
      "9 Test accuracy: 0.9502\n",
      "10 Test accuracy: 0.9528\n",
      "11 Test accuracy: 0.9549\n",
      "12 Test accuracy: 0.9566\n",
      "13 Test accuracy: 0.9589\n",
      "14 Test accuracy: 0.9602\n",
      "15 Test accuracy: 0.9612\n",
      "16 Test accuracy: 0.962\n",
      "17 Test accuracy: 0.9637\n",
      "18 Test accuracy: 0.9651\n",
      "19 Test accuracy: 0.9656\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dnn/batch_normalization/AssignMovingAvg:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Tensor 'dnn/batch_normalization/AssignMovingAvg_1:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Tensor 'dnn/batch_normalization_2/AssignMovingAvg:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Tensor 'dnn/batch_normalization_2/AssignMovingAvg_1:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Tensor 'dnn/batch_normalization_3/AssignMovingAvg:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Tensor 'dnn/batch_normalization_3/AssignMovingAvg_1:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_update_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'train/Momentum' type=NoOp>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그리고 tf.get_collection(tf.GraphKeys.UPDATE_OPS)을 수행하면, 정규화값들의 리스트가 리턴된다.\n",
    "\n",
    "- 뭐!? 그것은 MNIST에 대한 큰 정확성이 아닙니다. 물론, 더 길게 훈련하면 정확도는 훨씬 좋아질 것입니다. 그러나 그러한 얕은 네트워크에서 Batch Norm과 ELU는 매우 긍정적인 영향을 미치지 않을 것입니다.\n",
    "\n",
    "- Note that you could also make the training operation depend on the update operations:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "- 이렇게하면 트레이닝 중에 'training_op'을 평가하기만 하면 TensorFlow가 자동으로 업데이트 작업을 실행합니다.\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```\n",
    "\n",
    "- 한 가지 더 : 훈련 가능한 변수 목록이 모든 전역 변수 목록보다 짧음을 주목하십시오. 이동 평균은 훈련이 불가능한 변수이기 때문입니다. 미리 훈련 된 신경망 (아래 참조)을 재사용하려면 이러한 훈련 불가능한 변수를 잊어서는 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping (기울기 제한)\n",
    "\n",
    "- 폭발하는 그라데이션 문제를 줄이는 보편적인 기술은 backpropagation 중에 그라데이션을 제한하여 임계 값을 초과하지 않도록 하는 것입니다. \n",
    "- 이를 그라데이션 클리핑(제한)이라고합니다. 일반적으로 사람들은 이제 일괄 정규화를 선호하지만 그래디언트 클리핑을 구현하는 방법을 아는 것은 여전히 유용합니다.\n",
    "\n",
    "- TensorFlow에서 optimizer의 minimize() 함수는 그라디언트를 계산하고 적용하기 때문에 옵티마이저의 compute_gradients() 메서드를 먼저 호출 한 다음 clip_by_value() 함수를 사용하여 그라디언트를 클리핑하는 작업을 만들고 마지막으로 최적화 프로그램의 apply_gradients() 메소드를 사용하여 클리핑 된 그라디언트를 적용하는 연산을 작성하십시오.\n",
    "\n",
    "- 다양한 방법이 존재하지만 흔히 쓰이는 방법은 그래디언트의 L2 norm이 기준값을 초과할 때 (threshold / L2 norm)을 곱해주는 것입니다.\n",
    "\n",
    "- [Difference between tf.clip_by_value and tf.clip_by_global_norm for RNN's and how to decide max value to clip on?](https://stackoverflow.com/questions/44796793/difference-between-tf-clip-by-value-and-tf-clip-by-global-norm-for-rnns-and-how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.3138\n",
      "1 Test accuracy: 0.8003\n",
      "2 Test accuracy: 0.8805\n",
      "3 Test accuracy: 0.9037\n",
      "4 Test accuracy: 0.9122\n",
      "5 Test accuracy: 0.9196\n",
      "6 Test accuracy: 0.9241\n",
      "7 Test accuracy: 0.9299\n",
      "8 Test accuracy: 0.9329\n",
      "9 Test accuracy: 0.9388\n",
      "10 Test accuracy: 0.9432\n",
      "11 Test accuracy: 0.9451\n",
      "12 Test accuracy: 0.9454\n",
      "13 Test accuracy: 0.9483\n",
      "14 Test accuracy: 0.9525\n",
      "15 Test accuracy: 0.9513\n",
      "16 Test accuracy: 0.9565\n",
      "17 Test accuracy: 0.9583\n",
      "18 Test accuracy: 0.9561\n",
      "19 Test accuracy: 0.9605\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평소처럼 모든 훈련 단계에서 이 training_op을 실행하십시오. 그라디언트를 계산하고 -1.0에서 1.0 사이의 클립을 적용하고 적용합니다. 임계 값은 조정할 수있는 하이퍼 매개 변수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing pretrained layers\n",
    "\n",
    "- 매우 큰 DNN을 처음부터 훈련하는 것은 일반적으로 좋은 생각이 아닙니다. 대신 힘든 문제와 씨름하려고 하는 것과 비슷한 작업을 수행하는 기존 신경망을 찾아서 이 계층의 하위 계층을 재사용 해야 합니다. 이런 네트워크를 transfer learning이라고 합니다. 훈련 속도가 현저하게 빨라질뿐만 아니라 훈련 데이터도 훨씬 적게 듭니다.\n",
    "\n",
    "- 예를 들어 사진을 동물, 식물, 차량 및 일상 물체 등 100 가지 범주로 분류하도록 훈련된 DNN에 액세스 할 수 있다고 가정합니다. 이제 특정 차량 유형을 분류하기 위해 DNN을 훈련해야 합니다. 이러한 작업은 매우 유사하므로 첫 번째 네트워크의 일부를 재사용해야합니다 (그림 11-4 참조).\n",
    "\n",
    "![img11_1](img/ch11_6.png)\n",
    "\n",
    "- 새 작업의 입력 사진이 원래 작업에 사용 된 것과 동일한 크기가 아닌 경우 원래 모델에서 예상한 크기로 크기를 조정하기 위해 사전 처리 단계를 추가해야 합니다. 보다 일반적으로, 입력이 유사한 저수준 기능을 가지고 있는 경우에만 transfer learning이 잘 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "gradients_1/Shape\n",
      "gradients_1/Const\n",
      "gradients_1/Fill\n",
      "gradients_1/loss/loss_grad/Reshape/shape\n",
      "gradients_1/loss/loss_grad/Reshape\n",
      "gradients_1/loss/loss_grad/Shape\n",
      "gradients_1/loss/loss_grad/Tile\n",
      "gradients_1/loss/loss_grad/Shape_1\n",
      "gradients_1/loss/loss_grad/Shape_2\n",
      "gradients_1/loss/loss_grad/Const\n",
      "gradients_1/loss/loss_grad/Prod\n",
      "gradients_1/loss/loss_grad/Const_1\n",
      "gradients_1/loss/loss_grad/Prod_1\n",
      "gradients_1/loss/loss_grad/Maximum/y\n",
      "gradients_1/loss/loss_grad/Maximum\n",
      "gradients_1/loss/loss_grad/floordiv\n",
      "gradients_1/loss/loss_grad/Cast\n",
      "gradients_1/loss/loss_grad/truediv\n",
      "gradients_1/zeros_like\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients_1/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients_1/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_12/Minimum/y\n",
      "clip_by_value_12/Minimum\n",
      "clip_by_value_12/y\n",
      "clip_by_value_12\n",
      "clip_by_value_13/Minimum/y\n",
      "clip_by_value_13/Minimum\n",
      "clip_by_value_13/y\n",
      "clip_by_value_13\n",
      "clip_by_value_14/Minimum/y\n",
      "clip_by_value_14/Minimum\n",
      "clip_by_value_14/y\n",
      "clip_by_value_14\n",
      "clip_by_value_15/Minimum/y\n",
      "clip_by_value_15/Minimum\n",
      "clip_by_value_15/y\n",
      "clip_by_value_15\n",
      "clip_by_value_16/Minimum/y\n",
      "clip_by_value_16/Minimum\n",
      "clip_by_value_16/y\n",
      "clip_by_value_16\n",
      "clip_by_value_17/Minimum/y\n",
      "clip_by_value_17/Minimum\n",
      "clip_by_value_17/y\n",
      "clip_by_value_17\n",
      "clip_by_value_18/Minimum/y\n",
      "clip_by_value_18/Minimum\n",
      "clip_by_value_18/y\n",
      "clip_by_value_18\n",
      "clip_by_value_19/Minimum/y\n",
      "clip_by_value_19/Minimum\n",
      "clip_by_value_19/y\n",
      "clip_by_value_19\n",
      "clip_by_value_20/Minimum/y\n",
      "clip_by_value_20/Minimum\n",
      "clip_by_value_20/y\n",
      "clip_by_value_20\n",
      "clip_by_value_21/Minimum/y\n",
      "clip_by_value_21/Minimum\n",
      "clip_by_value_21/y\n",
      "clip_by_value_21\n",
      "clip_by_value_22/Minimum/y\n",
      "clip_by_value_22/Minimum\n",
      "clip_by_value_22/y\n",
      "clip_by_value_22\n",
      "clip_by_value_23/Minimum/y\n",
      "clip_by_value_23/Minimum\n",
      "clip_by_value_23/y\n",
      "clip_by_value_23\n",
      "GradientDescent_1/learning_rate\n",
      "GradientDescent_1/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent_1/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent_1/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent_1/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent_1\n",
      "eval/InTopK\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorBoard를 사용하여 그래프를 시각화하는 것이 훨씬 쉽습니다. 다음과 같은 해킹을 통해 Jupyter 내의 그래프를 시각화 할 수 있습니다 (브라우저에서 작동하지 않는 경우 FileWriter를 사용하여 그래프를 저장 한 다음 TensorBoard에서 시각화해야 함)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients_1/Shape&quot;\\n  input: &quot;gradients_1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/Fill&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients_1/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients_1/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_12/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_12/Minimum&quot;\\n  input: &quot;clip_by_value_12/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_13/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_13/Minimum&quot;\\n  input: &quot;clip_by_value_13/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_14/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_14/Minimum&quot;\\n  input: &quot;clip_by_value_14/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_15/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_15/Minimum&quot;\\n  input: &quot;clip_by_value_15/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_16/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_16/Minimum&quot;\\n  input: &quot;clip_by_value_16/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_17/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_17/Minimum&quot;\\n  input: &quot;clip_by_value_17/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_18/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_18/Minimum&quot;\\n  input: &quot;clip_by_value_18/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_19/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_19/Minimum&quot;\\n  input: &quot;clip_by_value_19/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_20/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_20/Minimum&quot;\\n  input: &quot;clip_by_value_20/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_21/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_21/Minimum&quot;\\n  input: &quot;clip_by_value_21/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_22/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_22/Minimum&quot;\\n  input: &quot;clip_by_value_22/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_23/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_23/Minimum&quot;\\n  input: &quot;clip_by_value_23/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_12&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_13&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_14&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_15&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_16&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_17&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_18&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_19&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_20&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_21&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_22&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_23&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent_1/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_outputs/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_outputs/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/InTopK&quot;\\n  op: &quot;InTopK&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;k&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/InTopK&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_1/tensor_names&quot;\\n  input: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_2/tensor_names&quot;\\n  input: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_3/tensor_names&quot;\\n  input: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_4/tensor_names&quot;\\n  input: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_5/tensor_names&quot;\\n  input: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_6/tensor_names&quot;\\n  input: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_7/tensor_names&quot;\\n  input: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_8/tensor_names&quot;\\n  input: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_9/tensor_names&quot;\\n  input: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_10/tensor_names&quot;\\n  input: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_11/tensor_names&quot;\\n  input: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필요한 연산을 알고 있으면 그래프의 get_operation_by_name () 또는 get_tensor_by_name () 메소드를 사용하여 핸들을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원래 모델을 작성한 사람이라면 작업에 매우 명확한 이름을 지정하고 문서화하여 모델을 재사용하는 사람들이 쉽게 작업 할 수 있습니다. \n",
    "\n",
    "- 또 다른 접근법은 사람들이 핸들을 얻고 자하는 모든 중요한 작업을 포함하는 모음을 만드는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게 하면 모델을 재사용하는 사람들은 다음과 같이 간단하게 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 세션을 시작하고 모델의 상태를 복원하고 데이터에 대한 훈련을 계속할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.961\n",
      "1 Test accuracy: 0.961\n",
      "2 Test accuracy: 0.9625\n",
      "3 Test accuracy: 0.9611\n",
      "4 Test accuracy: 0.9638\n",
      "5 Test accuracy: 0.9649\n",
      "6 Test accuracy: 0.9664\n",
      "7 Test accuracy: 0.963\n",
      "8 Test accuracy: 0.9665\n",
      "9 Test accuracy: 0.9668\n",
      "10 Test accuracy: 0.9664\n",
      "11 Test accuracy: 0.9671\n",
      "12 Test accuracy: 0.9678\n",
      "13 Test accuracy: 0.968\n",
      "14 Test accuracy: 0.969\n",
      "15 Test accuracy: 0.9686\n",
      "16 Test accuracy: 0.969\n",
      "17 Test accuracy: 0.9704\n",
      "18 Test accuracy: 0.9673\n",
      "19 Test accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또는 원본 그래프를 작성한 Python 코드에 액세스 할 수 있으면 import_meta_graph () 대신이 코드를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9613\n",
      "1 Test accuracy: 0.9619\n",
      "2 Test accuracy: 0.962\n",
      "3 Test accuracy: 0.9617\n",
      "4 Test accuracy: 0.9643\n",
      "5 Test accuracy: 0.9638\n",
      "6 Test accuracy: 0.9649\n",
      "7 Test accuracy: 0.9648\n",
      "8 Test accuracy: 0.9671\n",
      "9 Test accuracy: 0.9679\n",
      "10 Test accuracy: 0.9677\n",
      "11 Test accuracy: 0.9681\n",
      "12 Test accuracy: 0.969\n",
      "13 Test accuracy: 0.9685\n",
      "14 Test accuracy: 0.9686\n",
      "15 Test accuracy: 0.9695\n",
      "16 Test accuracy: 0.9679\n",
      "17 Test accuracy: 0.9694\n",
      "18 Test accuracy: 0.9691\n",
      "19 Test accuracy: 0.9709\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 하위 계층 만 다시 사용하고자 할 것입니다. import_meta_graph ()를 사용하는 경우 전체 그래프가 로드되지만 필요하지 않은 부분은 무시할 수 있습니다. \n",
    "\n",
    "- 이 예에서는 기존의 네 번째 숨겨진 레이어를 무시하고 사전에 배치 된 세 번째 레이어의 맨 위에 새로운 네 번째 숨겨진 레이어를 추가합니다. \n",
    "\n",
    "- 우리는 또한 새로운 출력 레이어,이 새로운 출력에 대한 손실 및이를 최소화하는 새로운 옵티 마이저를 구축합니다. \n",
    "\n",
    "- 또한 전체 그래프 (전체 이전 그래프와 새 연산 모두 포함)를 저장하고 초기화 작업을 통해 모든 새 변수를 초기화하는 또 다른 보호기가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And we can train this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9247\n",
      "1 Test accuracy: 0.9378\n",
      "2 Test accuracy: 0.9477\n",
      "3 Test accuracy: 0.9514\n",
      "4 Test accuracy: 0.9571\n",
      "5 Test accuracy: 0.9571\n",
      "6 Test accuracy: 0.9577\n",
      "7 Test accuracy: 0.9597\n",
      "8 Test accuracy: 0.9616\n",
      "9 Test accuracy: 0.9619\n",
      "10 Test accuracy: 0.9629\n",
      "11 Test accuracy: 0.9618\n",
      "12 Test accuracy: 0.963\n",
      "13 Test accuracy: 0.9657\n",
      "14 Test accuracy: 0.9643\n",
      "15 Test accuracy: 0.9659\n",
      "16 Test accuracy: 0.9664\n",
      "17 Test accuracy: 0.9672\n",
      "18 Test accuracy: 0.967\n",
      "19 Test accuracy: 0.9684\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원래 그래프를 작성한 Python 코드에 액세스 할 수 있다면 필요한 부분을 다시 사용하고 나머지 부분은 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그러나 미리 만들어진 모델을 복원하기 위해 하나의 `Saver`를 생성해야 합니다 (복원 할 변수의 목록을 제공하거나 그래프가 일치하지 않는다고 불평할 것입니다) 그리고 한 번만 새로운 모델을 저장하는 `Saver`를 한 번 만들어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9023\n",
      "1 Test accuracy: 0.93\n",
      "2 Test accuracy: 0.9392\n",
      "3 Test accuracy: 0.9428\n",
      "4 Test accuracy: 0.9485\n",
      "5 Test accuracy: 0.9512\n",
      "6 Test accuracy: 0.9517\n",
      "7 Test accuracy: 0.954\n",
      "8 Test accuracy: 0.9545\n",
      "9 Test accuracy: 0.9571\n",
      "10 Test accuracy: 0.9599\n",
      "11 Test accuracy: 0.9602\n",
      "12 Test accuracy: 0.9605\n",
      "13 Test accuracy: 0.9621\n",
      "14 Test accuracy: 0.9618\n",
      "15 Test accuracy: 0.9635\n",
      "16 Test accuracy: 0.963\n",
      "17 Test accuracy: 0.9645\n",
      "18 Test accuracy: 0.9651\n",
      "19 Test accuracy: 0.9662\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                      # not shown in the book\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,  # not shown\n",
    "                                                y: mnist.test.labels}) # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden1/bias': <tf.Variable 'hidden1/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " 'hidden1/kernel': <tf.Variable 'hidden1/kernel:0' shape=(784, 300) dtype=float32_ref>,\n",
       " 'hidden2/bias': <tf.Variable 'hidden2/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " 'hidden2/kernel': <tf.Variable 'hidden2/kernel:0' shape=(300, 50) dtype=float32_ref>,\n",
       " 'hidden3/bias': <tf.Variable 'hidden3/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " 'hidden3/kernel': <tf.Variable 'hidden3/kernel:0' shape=(50, 50) dtype=float32_ref>}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuse_vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.saver.Saver at 0x106b96cc0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing models from other frameworks\n",
    "\n",
    "- 다른 프레임 워크를 사용하여 모델을 학습 한 경우 수동으로 가중치를 로드해야 합니다 (예 : Theano 코드를 사용하여 Theano 코드를 사용하는 경우). \n",
    "\n",
    "- 그런 다음 해당 변수에 해당 값을 할당해야 합니다. 이것은 매우 지루할 수 있습니다. 예를 들어, 다음 코드는 다른 프레임 워크를 사용하여 훈련 된 모델의 첫 번째 숨겨진 레이어에서 가중치와 바이어스를 복사하는 방법을 보여줍니다.\n",
    "\n",
    "- 이 예제에서 재사용하고자 하는 각 변수에 대해 이니셜 라이저의 할당 연산을 찾고 초기화 값에 해당하는 두 번째 입력을 얻습니다. 이니셜 라이저를 실행할 때, 우리는 feed_dict를 사용하여 원하는 값으로 초기화 값을 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고 : tf.layers.dense () 함수로 만든 가중치 변수는 책과 같이 tf.contrib.layers.fully_connected ()를 사용할 때 \"가중치\"대신 \"커널\"이라고하고, 바이어스 대신 바이어스라고 부릅니다.\n",
    "\n",
    "- 처음에 이 책에서 사용된 다른 접근법은 전용 할당 노드와 전용 자리 표시자를 만드는 것입니다. 이것은 보다 장황하고 덜 효율적이지만, 보다 명확하게 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_collection ()을 사용하고 범위를 지정하여 변수에 대한 핸들을 얻을 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또는 그래프의 get_tensor_by_name () 메소드를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Lower Layers\n",
    "\n",
    "- 첫 번째 DNN의 하위 계층은 두 이미지 분류 작업에서 유용 할 수 있는 그림의 하위 수준 기능을 감지하여 이 계층을 그대로 재사용 할 수 있습니다.\n",
    "\n",
    "- 새 DNN을 학습 할 때 일반적으로 가중치를 \"잠그는\"것이 좋습니다. 하위 계층 가중치가 고정된 경우 상위 계층 가중치가 이동하기 쉽기 때문에 (이동하는 대상을 배울 필요가 없기 때문에) 트레이닝 중에 하위 레이어를 고정시키려면 가장 간단한 해결책은 하위 레이어의 변수를 제외하고 최적화 할 트레이닝 변수 목록을 지정하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.8986\n",
      "1 Test accuracy: 0.931\n",
      "2 Test accuracy: 0.937\n",
      "3 Test accuracy: 0.9414\n",
      "4 Test accuracy: 0.9436\n",
      "5 Test accuracy: 0.9482\n",
      "6 Test accuracy: 0.9494\n",
      "7 Test accuracy: 0.9523\n",
      "8 Test accuracy: 0.9517\n",
      "9 Test accuracy: 0.9522\n",
      "10 Test accuracy: 0.9536\n",
      "11 Test accuracy: 0.9539\n",
      "12 Test accuracy: 0.9534\n",
      "13 Test accuracy: 0.9546\n",
      "14 Test accuracy: 0.9537\n",
      "15 Test accuracy: 0.9552\n",
      "16 Test accuracy: 0.9553\n",
      "17 Test accuracy: 0.9551\n",
      "18 Test accuracy: 0.9555\n",
      "19 Test accuracy: 0.9554\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training code is exactly the same as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9037\n",
      "1 Test accuracy: 0.9321\n",
      "2 Test accuracy: 0.9399\n",
      "3 Test accuracy: 0.9437\n",
      "4 Test accuracy: 0.9474\n",
      "5 Test accuracy: 0.9491\n",
      "6 Test accuracy: 0.9498\n",
      "7 Test accuracy: 0.9493\n",
      "8 Test accuracy: 0.9516\n",
      "9 Test accuracy: 0.9519\n",
      "10 Test accuracy: 0.9531\n",
      "11 Test accuracy: 0.9535\n",
      "12 Test accuracy: 0.9531\n",
      "13 Test accuracy: 0.9535\n",
      "14 Test accuracy: 0.9525\n",
      "15 Test accuracy: 0.9536\n",
      "16 Test accuracy: 0.9528\n",
      "17 Test accuracy: 0.9547\n",
      "18 Test accuracy: 0.955\n",
      "19 Test accuracy: 0.9557\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching the frozen layers\n",
    "\n",
    "- 고정된 레이어는 변경되지 않으므로 각 훈련 인스턴스에 대해 맨 위에 고정된 레이어의 출력을 캐시 할 수 있습니다. \n",
    "\n",
    "- 훈련은 전체 데이터 세트를 여러 번 거치기 때문에 훈련 인스턴스 별로 한 번만 (한 시대 당 한 번이 아닌) 고정 된 레이어를 거쳐야하기 때문에 엄청난 속도 향상을 얻을 수 있습니다. 예를 들어, 먼저 하위 계층을 통해 전체 훈련 세트를 실행할 수 있습니다 (충분한 RAM이 있다고 가정)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런 다음 훈련 인스턴스의 일괄 처리를 작성하는 대신 훈련 중에 숨겨진 계층 2의 출력 배치를 작성하여 훈련 작업에 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9032\n",
      "1 Test accuracy: 0.9323\n",
      "2 Test accuracy: 0.9423\n",
      "3 Test accuracy: 0.9448\n",
      "4 Test accuracy: 0.947\n",
      "5 Test accuracy: 0.9475\n",
      "6 Test accuracy: 0.951\n",
      "7 Test accuracy: 0.9509\n",
      "8 Test accuracy: 0.9514\n",
      "9 Test accuracy: 0.9522\n",
      "10 Test accuracy: 0.9514\n",
      "11 Test accuracy: 0.9523\n",
      "12 Test accuracy: 0.9523\n",
      "13 Test accuracy: 0.9539\n",
      "14 Test accuracy: 0.9537\n",
      "15 Test accuracy: 0.9534\n",
      "16 Test accuracy: 0.9545\n",
      "17 Test accuracy: 0.9537\n",
      "18 Test accuracy: 0.9537\n",
      "19 Test accuracy: 0.9547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})\n",
    "    h2_cache_test = sess.run(hidden2, feed_dict={X: mnist.test.images}) # not shown in the book\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_test, # not shown\n",
    "                                                y: mnist.test.labels})  # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                    # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막 줄은 이전에 정의 된 훈련 과정 (레이어 1과 레이어 2가 고정되어 있음)을 실행하고 두 번째 숨겨진 레이어 (해당 배치의 타겟)의 출력 배치에 피드를 제공합니다. TensorFlow에 숨겨진 레이어 2의 출력을 제공하기 때문에 이 레이어를 평가하지는 않습니다 (또는 이 레이어가 종속 된 모든 노드)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking, dropping or replacing the upper layers\n",
    "\n",
    "- 원본 모델의 출력 레이어는 일반적으로 새 작업에 유용하지 않을 가능성이 높기 때문에 대개 교체해야 하며 새 작업에 대해 올바른 출력 개수를 갖지 못할 수도 있습니다.\n",
    "\n",
    "- 마찬가지로 원래 모델의 상위 숨겨진 레이어는 새 작업에 가장 유용한 상위 수준 기능이 원래 작업에 가장 유용한 수준과 크게 다를 수 있기 때문에 하위 레이어만큼 유용하지는 않습니다. 재사용 할 레이어의 수를 찾고 싶습니다.\n",
    "\n",
    "- 복사 된 모든 레이어를 먼저 고정시키고 모델을 교육하고 성능을 확인하십시오. 그런 다음 최상위 숨겨진 레이어 중 하나 또는 두 개를 해제하여 backpropagation을 조정하여 성능이 향상되는지 확인하십시오. 훈련 데이터가 많을수록 고정 해제 할 수 있는 레이어가 늘어납니다.\n",
    "\n",
    "- 그래도 성능이 좋지 않고 훈련 데이터가 거의 없으면 상단 숨겨진 레이어를 놓고 나머지 숨겨진 레이어를 모두 다시 고정하십시오. 재사용 할 레이어를 찾을 때까지 반복 할 수 있습니다. 많은 훈련 데이터가 있는 경우, 숨겨진 레이어를 삭제하는 대신 상단의 숨겨진 레이어를 대체하거나 숨겨진 레이어를 더 추가 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model zoos\n",
    "\n",
    "- 힘든 문제와 씨름하고 싶은 것과 유사한 작업을 위해서 훈련된 신경망을 찾을 수 있습니까? 가장 먼저 살펴볼 모델은 분명히 자신의 모델 카탈로그에 있습니다. 이것은 모든 모델을 저장하고 나중에 쉽게 검색 할 수 있도록 구성하는 좋은 이유 중 하나입니다. \n",
    "\n",
    "- 또 다른 옵션은 model zoo을 검색하는 것입니다. 많은 사람들이 기계를 배웁니다. 다양한 작업에 대한 학습 모델을 작성하여 사전에 공개 된 모델을 대중에게 공개하십시오. \n",
    "\n",
    "- TensorFlow는 자체 model zoo을 https://github.com/tensorflow/models 에서 사용할 수 있습니다. 특히 VGG, Inception, ResNet (제 13 장, 모델 / 슬림 디렉토리 확인 참조)과 같은 최첨단 이미지 분류 그물을 포함하며, 코드, 사전 계획된 모델 및 다운로드 도구를 포함합니다. 인기있는 이미지 데이터 세트.\n",
    "\n",
    "- 또 다른 유명한 model zoo는 Caffe의 Model-Zoo입니다. 또한 다양한 데이터 세트 (예 : ImageNet, Places Database, CIFAR10 등)에 대한 훈련을 받은 많은 컴퓨터 비전 모델 (예 : LeNet, AlexNet, ZFNet, GoogleNet, VGGNet)을 포함합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised pretraining\n",
    "\n",
    "- 많은 양의 훈련 데이터가 없는 복잡한 작업을 하고 싶다고 가정하지만, 불행히도 유사한 작업에 대해 훈련받은 모델을 찾을 수는 없습니다. \n",
    "\n",
    "- 희망을 잃지 마십시오! 첫째, 물론 더 많은 훈련용 데이터를 수집해야 하지만 너무 힘들거나 너무 비싸면 미리 숙련되지 않은 사전 훈련을 수행 할 수 있습니다 (그림 11-5 참조) \n",
    "\n",
    "- 레이블이 없는 훈련을 많이 받은 경우 데이터를 사용하는 경우 제한된 볼츠만 머신 (RBM, 부록 E 참조) 또는 Autoencoders와 같은 감독되지 않은 탐지 알고리즘을 사용하여 레이어를 하나씩 차례로 조정할 수 있습니다. \n",
    "\n",
    "- 각 계층은 이전에 훈련된 계층의 출력에 대해 학습됩니다 (훈련된 계층을 제외한 모든 계층이 잠김). 모든 계층을 이 방법으로 훈련하면 감독 학습 (예 : 역전파)을 사용하여 네트워크를 미세 조정할 수 있습니다.\n",
    "\n",
    "- 이는 다소 길고 지루한 과정이지만 종종 잘 작동합니다. 실제로 Geoffrey Hinton과 그의 팀이 2006 년에 사용한이 기법은 신경 네트워크의 부활과 깊은 학습의 성공으로 이어졌습니다. 2010 년까지는 일반적으로 RBM을 사용하는 비 숙련 사전 훈련 (pretraining)이 심층 그물망의 표준이었으며 소실점이 있는 그라디언트 문제가 완화된 후에 순전히 역전파를 사용하여 DNN을 훈련하는 것이 훨씬 더 일반적이었습니다. \n",
    "\n",
    "- 그러나 일반적으로 RBM이 아닌 Autoencoders를 사용하는 감독되지 않은 사전 처리는 해결해야 할 복잡한 작업이 있고 재사용 할 수있는 비슷한 모델이 없으며 레이블이 없는 훈련 데이터이지만 레이블이 없는 훈련 데이터가 많습니다.\n",
    "\n",
    "![img11_1](img/ch11_7.png)\n",
    "\n",
    "\n",
    "### Pretraining on an auxiliary task\n",
    "\n",
    "- 마지막 옵션은 레이블이 지정된 훈련 데이터를 쉽게 얻거나 생성 할 수 있는 보조 작업에서 첫 번째 신경망을 학습한 다음 실제 작업을 위해 해당 네트워크의 하위 계층을 재사용하는 것입니다. 첫 번째 신경망의 하위 계층은 두 번째 신경망에서 재사용 할 수 있는 기능 감지기를 배웁니다.\n",
    "\n",
    "- 예를 들어, 얼굴을 인식할 수 있는 시스템을 구축하려는 경우 각 개인에 대한 사진이 몇 개 있을 수 있습니다. 각 사람의 사진 수백 장을 모으는 것은 실용적이지 못합니다. 그러나 인터넷에서 임의의 사람들의 사진을 많이 수집하고 두 개의 서로 다른 사진이 동일한 사람을 특징으로 하는지 여부를 감지하는 첫 번째 신경 네트워크를 훈련 할 수 있습니다. 이러한 네트워크는 얼굴에 대한 우수한 기능 감지기를 학습하므로 하위 레이어를 재사용하면 학습 데이터가 거의 없는 우수한 얼굴 분류기를 학습 할 수 있습니다.\n",
    "\n",
    "- 레이블이 없는 훈련 사례를 수집하는 것은 종종 저렴하지만 레이블을 지정하는 데 비용이 많이 듭니다. 이러한 상황에서 공통적인 기술은 모든 훈련 시험을 \"우수\"로 표시 한 다음 우수한 교육 인스턴스를 손상시켜 많은 새로운 훈련 인스턴스를 생성하고 이러한 손상된 인스턴스를 \"불량\"으로 표시하는 것입니다. \n",
    "\n",
    "- 그런 다음 인스턴스를 좋거나 나쁘게 분류하기 위해 첫 번째 신경망을 훈련시킬 수 있습니다. 예를 들어, 수백만 개의 문장을 다운로드하여 \"good\"이라고 표시 한 다음 각 문장에서 임의로 단어를 변경하고 결과 문장에 \"bad\"라고 레이블을 지정할 수 있습니다. \n",
    "\n",
    "- 신경 회로망이 \"The dog sleeps\"은 좋은 문장이지만 \"dog dog\"은 나쁘다는 것을 알 수 있다면 언어에 대해 꽤 많이 알고있을 것입니다. 하위 계층을 재사용하면 많은 언어 처리 작업에서 도움이 될 것입니다.\n",
    "\n",
    "- 또 다른 접근 방법은 첫 번째 네트워크를 교육하여 각 훈련 인스턴스에 대한 점수를 출력하고 좋은 인스턴스의 점수가 나쁜 인스턴스의 점수보다 적어도 어느 정도 더 큰지 확인하는 비용 함수를 사용하는 것입니다. 이것을 최대 마진 학습이라고합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster optimizers\n",
    "\n",
    "- 매우 큰 심층 신경 네트워크를 훈련하는 것은 고통스럽게 느려질 수 있습니다. 지금까지 우리는 훈련을 가속화하고 더 나은 솔루션에 도달하는 네 가지 방법, 즉 연결 가중치에 대해 좋은 초기화 전략을 적용하고, 좋은 활성화 함수를 사용하고, 일괄 정규화를 사용하고 사전 네트워크의 일부를 재사용하는 방법을 보았습니다. \n",
    "\n",
    "- 또 다른 큰 속도 향상은 일반 Gradient Descent 최적화 알고리즘보다 빠른 옵티 마이저를 사용함에 있습니다. \n",
    "\n",
    "- 이 섹션에서는 모멘텀 최적화, Nesterov Accelerated Gradient, AdaGrad, RMSProp, 그리고 마지막으로 Adam 최적화와 같은 가장 인기있는 것들을 제시 할 것입니다.\n",
    "\n",
    "- #### Spoiler 경고 :이 섹션의 결론은 거의 항상 Adam 최적화를 사용해야 한다는 것입니다.\n",
    "\n",
    "- 따라서 작동 원리에 신경쓰지 않는다면 GradientDescentOptimizer를 AdamOptimizer로 바꾸고 다음 섹션으로 건너 뜁니다. 이 작은 변화만으로도 훈련은 일반적으로 몇 배 더 빠릅니다. \n",
    "\n",
    "- 그러나 Adam 최적화에는 조정할 수있는 세 가지 하이퍼 매개 변수 (학습 속도 포함)가 있습니다. 기본 값은 일반적으로 잘 작동하지만 조정해야 할 경우 수행하는 작업을 파악하는 것이 도움이 될 수 있습니다. \n",
    "\n",
    "- Adam 최적화는 다른 최적화 알고리즘의 여러 아이디어를 결합하므로 먼저 이러한 알고리즘을 살펴 보는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimization\n",
    "\n",
    "- [Gradient Descent Optimization Algorithms 정리](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)\n",
    "\n",
    "\n",
    "- 볼링 공이 매끄러운 표면에서 완만한 경사면을 굴러 떨어지는 것을 상상해보십시오. 천천히 시작하지만 마침내 끝 속도 (마찰이나 공기 저항이있는 경우)에 도달 할 때까지 빠르게 기세를 회복합니다.\n",
    "\n",
    "- 이와는 반대로, 1964 년 Boris Polyak에 의해 제안 된 모멘텀 최적화의 가장 단순한 개념입니다. 대조적으로, 규칙적인 Gradient Descent는 슬로프를 따라 작은 규칙적인 단계를 거치므로 바닥에 도달하는 데 더 많은 시간이 걸릴 것입니다. 이전 그라디언트가 무엇인지 신경 쓰지 않습니다. \n",
    "\n",
    "- 로컬 그래디언트가 작으면 매우 느리게 이동합니다. 모멘텀 최적화는 이전 그라디언트가 무엇인지에 대해 큰 관심을 가지고 있습니다. 각 반복에서 모멘텀 벡터에 로컬 그라디언트를 추가하고 (학습 속도 η로 곱함)이 모멘텀 벡터를 간단히 빼서 가중치를 업데이트합니다 (식 11- 4). \n",
    "\n",
    "- 다시 말해서, 그레디언트는 속도가 아닌 가속도로 사용됩니다. 어떤 종류의 마찰 메커니즘을 시뮬레이션하고 운동량이 너무 커지지 않도록하기 위해이 알고리즘은 0 (고 마찰)과 1 (마찰 없음) 사이에서 설정해야 하는 운동량이라고 하는 새로운 하이퍼 매개 변수 β를 도입합니다. 일반적인 운동량 값은 0.9입니다.\n",
    "\n",
    "- 그래디언트가 일정하게 유지되면 터미널 속도 (즉, 가중치 업데이트의 최대 크기)가 학습 속도 η에 1을 곱한 배율과 같음을 쉽게 확인할 수 있습니다. \n",
    "\n",
    "- 예를 들어, β = 0 일 때. 단자 1-β 속도는 기울기의 10 배와 학습 속도를 곱한 것과 같기 때문에 모멘텀 최적화는 Gradient Descent보다 10 배 빠르게 끝납니다! \n",
    "\n",
    "- 이를 통해 모멘텀 최적화가 그라디언트 하강보다 훨씬 빠르게 고원에서 벗어날 수 있습니다. 가파른 경사를 지나치게 빠르지만, 매우 오랜 시간이 소요됩니다. 반대로 모멘텀 최적화는 계곡의 바닥이 최적에 도달 할 때까지 계곡의 바닥을 더 빠르고 더 빠르게 굴립니다. 일괄 정규화를 사용하지 않는 딥 뉴럴 네트워크에서 상위 레이어는 종종 매우 다른 스케일의 입력을 갖기 때문에 모멘텀 최적화를 사용하면 많은 도움이됩니다. 그것은 또한 과거의 지역 최적을 롤백하는 것을 도울 수 있습니다.\n",
    "\n",
    "```\n",
    "모멘텀으로 인해 옵티마이저는 약간 오버 슛하고, 다시 돌아오고, 다시 오버 슈트하고, 최소로 안정되기 전에 여러 번 진동 할 수 있습니다. 이것은 시스템에서 약간의 마찰을 갖는 것이 좋은 이유 중 하나입니다. 이러한 진동을 없애고 수렴 속도를 높입니다.\n",
    "```\n",
    "\n",
    "- 모멘텀 최적화의 한 가지 단점은 조정할 또 다른 하이퍼 파라미터를 추가한다는 것입니다. 그러나 운동량 값 0.9은 일반적으로 실제로 잘 작동하며 거의 항상 그라데이션 하강보다 빠릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Momentum optimization\n",
    "\n",
    "- 1983 년 Yurii Nesterov가 모멘텀 최적화에 대한 작은 변형을 제안 했으므로 바닐라 모멘텀 최적화보다 항상 빠릅니다. \n",
    "\n",
    "- Nesterov 가속도 그레디언트 (Nesterov Accelerated Gradient, NAG)라고도 불리는 네 스테 로프 운동량 최적화의 아이디어는 지역 위치가 아닌 운동량의 방향으로 약간 앞선 비용 함수의 기울기를 측정하는 것입니다 (식 11-5 참조). 바닐라 모멘텀 최적화의 유일한 차이는 기울기가 θ가 아닌 θ + β에서 측정된다는 점입니다.\n",
    "\n",
    "- 이 작은 미세 조정은 일반적으로 운동량 벡터가 올바른 방향 (즉, 최적 방향)으로 향하기 때문에 그라디언트를 사용하는 대신 그 방향으로 조금 더 측정 한 그라디언트를 사용하는 것이 약간 정확합니다.\n",
    "\n",
    "![img11_1](img/ch11_8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "- 긴 사발 문제를 다시 생각해보십시오. 경사 경사면은 가장 빠른 경사면을 따라 빨리 시작한 다음 천천히 계곡 아래쪽으로 내려갑니다. 알고리즘이 초기에 이를 감지하고 그 방향을 수정하여 전역 최적을 향해 좀 더 지적 할 수 있다면 좋을 것입니다.\n",
    "\n",
    "- AdaGrad 알고리즘은 가장 가파른 차원을 따라 그래디언트 벡터를 축소함으로써 이를 달성합니다.\n",
    "\n",
    "- 첫 번째 단계는 벡터에 그라디언트의 제곱을 누적합니다\n",
    "\n",
    "- 두 번째 단계는 Gradient Descent와 거의 동일하지만 하나의 큰 차이점이 있습니다 : 그라디언트 벡터는 factor +로 축소됩니다.\n",
    "\n",
    "- 간단히 말해서,이 알고리즘은 학습 속도를 떨어뜨리지만 가파른 차원의 경우보다 가파른 차원의 경우 더 빠릅니다. 이를 적응 학습 속도라고합니다. 결과 업데이트를 전역 최적화에보다 직접적으로 가리킬 수 있습니다 (그림 11-7 참조). 하나의 추가적인 이점은 학습 속도 hyperparameter η의 조정이 훨씬 덜 필요하다는 것입니다.\n",
    "\n",
    "![img11_1](img/ch11_9.png)\n",
    "\n",
    "- AdaGrad는 종종 간단한 2 차 문제에 대해 잘 수행하지만, 불행히도 신경 네트워크를 훈련 할 때 너무 일찍 종료합니다. \n",
    "\n",
    "- 학습 속도가 너무 많이 내려 가서 알고리즘이 전역 최적에 도달하기 전에 완전히 멈추는 경우가 있습니다. \n",
    "\n",
    "- 따라서 TensorFlow에 AdagradOptimizer가 있어도 Deep Neural Networks를 학습하는 데 사용해서는 안됩니다 (선형 회귀와 같은 간단한 작업에서는 효율적 일 수 있습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "- AdaGrad가 너무 느려지면서 글로벌 최적으로 수렴하지 못하는 경우에도 RMSProp 알고리즘은 가장 최근 반복의 그라디언트만 수집함으로써 이를 수정합니다 (훈련 시작 이후의 모든 그라디언트와 대조적으로). 이것은 첫 번째 단계에서 지수 붕괴를 사용하여 수행합니다 (식 11-7 참조)\n",
    "\n",
    "- 감쇠율 β는 전형적으로 0.9로 설정된다. 네, 다시 한번 새로운 하이퍼 파라미터입니다. 그러나이 디폴트 값은 종종 잘 작동하므로 튜닝을 전혀 할 필요가 없습니다.\n",
    "\n",
    "- 매우 간단한 문제를 제외하고이 옵티마이 저는 거의 항상 AdaGrad보다 훨씬 뛰어납니다. 또한 일반적으로 모멘텀 최적화 및 Nesterov Accelerated Gradients보다 뛰어납니다. 실제로 Adam 최적화가 이루어질 때까지 많은 연구자가 선호하는 최적화 알고리즘이었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimization\n",
    "\n",
    "- Adam (적응형 모멘트 추정)은 Momentum optimization 및 RMSProp의 아이디어를 결합합니다. \n",
    "\n",
    "- 모멘텀 최적화와 마찬가지로 기하 급수적으로 감소하는 과거 그라디언트 평균을 추적하며 RMSProp과 마찬가지로 과거 제곱 된 그라디언트의 기하 급수적으로 감소하는 평균을 추적합니다\n",
    "\n",
    "- 1, 2, 5 단계 만 보면 모멘텀 최적화와 RMSProp과의 유사성을 알 수 있습니다. 유일한 차이점은 1 단계는 기하 급수적으로 감소하는 합계가 아닌 지수 적으로 감쇠하는 평균을 계산한다는 것입니다. 그러나 이는 상수 요소를 제외하고는 실제로 동일합니다.\n",
    "\n",
    "- 3 단계와 4 단계는 다소 기술적 인 세부 사항입니다. 이후 및 0으로 초기화되면 훈련 시작시 0에 편향됩니다. 따라서이 두 단계는 훈련을 높이는 데 도움이됩니다.\n",
    "\n",
    "- 운동량 감쇠 상한값 β1은 일반적으로 0.9로 초기화되는 반면,(decay hyperparameter) β2는 종종 0.999로 초기화된다. \n",
    "\n",
    "- 사실, Adam은 AdaGrad 및 RMSProp과 같은 적응 학습 속도 알고리즘이기 때문에 학습 속도 초 파라미터 η의 조정이 덜 필요합니다. 기본값 인 η = 0.001을 자주 사용하여 Adam을 Gradient Descent보다 더 쉽게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "위에서 설명한 모든 최적화 기술은 1 차 부분 파생 상품 (Jacobians)에만 의존합니다. 최적화 문헌에는 2 차 부분 파생 상품 (헤 시안)을 기반으로 한 놀라운 알고리즘이 포함되어 있습니다. 불행히도, 이러한 알고리즘은 매우\n",
    "출력 당 n2 개의 헤 시안 (n은 매개 변수의 수)이 있기 때문에 딥 뉴럴 네트워크에 적용하기가 어렵습니다. DNN에는 일반적으로 수만 개의 매개 변수가 있기 때문에 2 차 최적화 알고리즘은 종종 메모리에 적합하지도 않으며 때로는 헤세 인을 계산하는 것이 너무 느립니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training sparse models\n",
    "- 위에 제시된 모든 최적화 알고리즘은 밀도가 높은 모델을 생성하므로 대부분의 매개 변수가 0이 아닙니다. 런타임에 아주 빠른 모델이 필요하거나 메모리를 적게 사용해야하는 경우 스파스 모델로 끝내는 것이 좋습니다.\n",
    "\n",
    "- 이를 달성하는 한 가지 간단한 방법은 평소처럼 모델을 훈련시킨 다음 작은 가중치를 없애는 것입니다 (0으로 설정).\n",
    "\n",
    "- 또 다른 옵션은 학습 도중 강력한 정규화를 적용하는 것입니다. 이는 최적화 알고리즘을 가능한 한 많은 가중치로 제로 아웃시킵니다.\n",
    "\n",
    "- 그러나 어떤 경우에는 이러한 기술이 불충분하게 남아있을 수 있습니다. 마지막 옵션은 Y. Nesterov15가 제안한 기술인 FTRL (Follow E Regularized Leader)이라고하는 듀얼 평균화를 적용하는 것입니다. l1 정규화와 함께 사용될 때,이 기법은 종종 매우 드문 드문 한 모델로 이어집니다. TensorFlow는 FTRLOptimizer 클래스에서 FTRL-근사치 이라는 변형 된 FTRL을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling\n",
    "\n",
    "- 최적의 학습 속도를 찾는 것이 까다로울 수 있습니다. 너무 높게 설정하면 훈련이 실제로 분기될 수 있습니다 (4 장에서 논의했듯이). 너무 낮게 설정하면 훈련은 결국 최적으로 수렴되지만 매우 오랜 시간이 걸립니다. \n",
    "\n",
    "- 약간 높게 설정하면 처음에는 매우 빠르게 진행되지만 AdaGrad, RMSProp 또는 Adam과 같은 적응 학습 속도 최적화 알고리즘을 사용하지 않는 한 최적으로 춤을 추며 끝나지 않습니다. 그런 다음 정착하는 데 시간이 걸릴 수 있음). 컴퓨팅 예산이 제한되어 있다면 제대로 수렴하기 전에 훈련을 중단해야만 최적의 솔루션이 될 수 있습니다.\n",
    "\n",
    "![img11_1](img/ch11_10.png)\n",
    "\n",
    "- 다양한 학습 속도를 사용하고 학습 곡선을 비교하는 단 몇 개의 신기원 동안 네트워크를 여러 번 훈련하면 상당히 좋은 학습 속도를 찾을 수 있습니다. 이상적인 학습 속도는 빠르게 배우고 좋은 해결책으로 수렴 할 것입니다.\n",
    "\n",
    "- 그러나 일정한 학습 속도보다 더 잘할 수 있습니다. 높은 학습 속도로 시작한 다음 빠르게 진행하지 않으면 학습 속도를 줄이면 최적의 상수 학습 속도보다 빠른 속도로 좋은 솔루션에 도달 할 수 있습니다. 훈련 중 학습 률을 줄이기위한 여러 가지 전략이 있습니다. 이러한 전략을 학습 일정이라고 합니다.\n",
    "\n",
    "\n",
    "    1. 미리 결정된 piecewise constant learning rate : 예를 들어 학습 률을 처음에는 η 0 = 0.1, 50 epoch 후에는 η 1 = 0.001로 설정합니다. 이 솔루션이 효과적일 수는 있지만 올바른 학습 속도와 사용시기를 찾기 위해 주변을 둘러보아야하는 경우가 종종 있습니다.\n",
    "\n",
    "    2. 성능 스케줄링 : N 단계마다 유효성 검증 오류를 측정하고 (Early Stopping과 동일) 오류가 발생하지 않을 때 λ만큼 학습률을 낮춥니다.\n",
    "\n",
    "    3. 지수적 스케줄링 : 학습 속도를 반복 횟수 t의 함수로 설정합니다. ηt = η0 10 -t/r승. 이것은 훌륭하게 작동하지만 η0와 r을 튜닝해야합니다. 학습 속도는 매 단계마다 10 배 떨어집니다.\n",
    "\n",
    "    4. 전력 스케줄링 : 학습 속도를 η t = η0 (1 + t / r )-c승 으로 설정한다. 하이퍼 매개 변수 c는 일반적으로 1로 설정됩니다. 이는 지수적 스케줄링과 유사하지만 학습 속도가 훨씬 느려집니다.\n",
    "    \n",
    "  \n",
    "- A. Senior 등의 2013 년 논문 모멘텀 최적화를 사용하여 음성 인식을 위한 심층 신경 네트워크를 학습 할 때 가장 인기있는 학습 일정의 성능을 비교했습니다.\n",
    "\n",
    "- 이 설정에서 성능 스케줄링과 지수 스케줄링이 모두 잘 수행되었지만 지수 스케줄링이 더 좋습니다. 구현하기 쉽고, 조정하기 쉽고, 최적의 솔루션보다 약간 빠른 속도로 변환되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9579\n",
      "1 Test accuracy: 0.9686\n",
      "2 Test accuracy: 0.9739\n",
      "3 Test accuracy: 0.9799\n",
      "4 Test accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하이퍼 매개 변수 값을 설정 한 후 현재 트레이닝 반복 번호를 추적하기 위해 훈련이 불가능한 변수 global_step (0으로 초기화 됨)을 생성합니다. \n",
    "\n",
    "- 다음 지수 붕괴 학습 속도를 정의합니다 (η0 = 0.1 및 r = 10,000)에 TensorFlow의 exponential_decay() 함수를 사용합니다. \n",
    "\n",
    "- 다음 이 감소하는 학습 속도를 사용하여 최적화 도구 (이 예제에서는 MomentumOptimizer)를 만듭니다.\n",
    "\n",
    "- 마지막으로 optimizer의 minimize() 메소드를 호출하여 training 연산을 생성합니다 : global_step 변수를 전달하기 때문에, incremental을 친절하게 처리 할 것입니다. \n",
    "\n",
    "- AdaGrad, RMSProp 및 Adam 최적화는 교육 중 학습 속도를 자동으로 줄이므로 추가 학습 일정을 추가 할 필요가 없습니다. 지수적 감쇠 또는 성능 스케줄링을 사용하는 다른 최적화 알고리즘의 경우 컨버전스 속도가 상당히 빨라질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Overfitting Through Regularization\n",
    "\n",
    "- 딥 뉴럴 네트워크는 일반적으로 수십만 개의 연결을 가지고 있으며 때로는 수백만 개가 연결됩니다. 매개 변수가 너무 많아서 네트워크에는 엄청난 자유가 있으며 복잡한 데이터 세트에도 매우 적합합니다. 그러나 이 큰 유연성은 또한 훈련 세트를 지나치게 맞추는 경향이 있음을 의미합니다.\n",
    "\n",
    "#### $\\ell_1$ and $\\ell_2$ regularization\n",
    "\n",
    "- Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, 우리는 계층 가중치에 대한 핸들을 얻고 일반적인 교차 엔트로피 손실과 $\\ell_1 $ 손실의 합계 (즉, 가중치의 절대 값)와 같은 총 손실을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8343\n",
      "1 Test accuracy: 0.8726\n",
      "2 Test accuracy: 0.8832\n",
      "3 Test accuracy: 0.8899\n",
      "4 Test accuracy: 0.8959\n",
      "5 Test accuracy: 0.8986\n",
      "6 Test accuracy: 0.9011\n",
      "7 Test accuracy: 0.9032\n",
      "8 Test accuracy: 0.9046\n",
      "9 Test accuracy: 0.9047\n",
      "10 Test accuracy: 0.9065\n",
      "11 Test accuracy: 0.9059\n",
      "12 Test accuracy: 0.9072\n",
      "13 Test accuracy: 0.9072\n",
      "14 Test accuracy: 0.9069\n",
      "15 Test accuracy: 0.9071\n",
      "16 Test accuracy: 0.9064\n",
      "17 Test accuracy: 0.9071\n",
      "18 Test accuracy: 0.9068\n",
      "19 Test accuracy: 0.9063\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또는 tf.layers.dense() 함수에 정규화 함수를 전달할 수 있습니다. tf.layers.dense() 함수는 이를 사용하여 정규화 손실을 계산하는 연산을 생성하고 이러한 연산을 정규화 손실 수집에 추가합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음으로, 동일한 인수를 반복해서 반복하지 않도록 Python partial () 함수를 사용합니다. 커널 regularizer 인수를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we must add the regularization losses to the base loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8298\n",
      "1 Test accuracy: 0.8778\n",
      "2 Test accuracy: 0.8917\n",
      "3 Test accuracy: 0.9017\n",
      "4 Test accuracy: 0.9068\n",
      "5 Test accuracy: 0.9103\n",
      "6 Test accuracy: 0.9125\n",
      "7 Test accuracy: 0.9137\n",
      "8 Test accuracy: 0.9148\n",
      "9 Test accuracy: 0.9175\n",
      "10 Test accuracy: 0.9176\n",
      "11 Test accuracy: 0.9184\n",
      "12 Test accuracy: 0.9191\n",
      "13 Test accuracy: 0.9183\n",
      "14 Test accuracy: 0.9195\n",
      "15 Test accuracy: 0.92\n",
      "16 Test accuracy: 0.9181\n",
      "17 Test accuracy: 0.9184\n",
      "18 Test accuracy: 0.9181\n",
      "19 Test accuracy: 0.9174\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "- 깊은 신경 네트워크를 위한 가장 인기있는 정규화 기법은 틀림없이 Dropout입니다. 2012년 G. Hinton에 의해 제안되었고 N. Srivastava 등의 논문에 자세히 설명되어 있으며 매우 성공적인 것으로 입증되었습니다. \n",
    "\n",
    "- 최첨단 신경망도 1-2%의 정확도를 가집니다. 부스트는 드롭 아웃을 추가하면 됩니다. 이것은 많이 들리지는 않지만, 모델이 이미 95 %의 정확도를 가지고있을 때, 2% 정확도 향상을 얻는 것은 오류율을 거의 40 % 떨어 뜨린다는 것을 의미합니다 (5% 오차에서 대략 3%).\n",
    "\n",
    "- 상당히 간단한 알고리즘입니다. 모든 훈련 단계에서 모든 뉴런 (입력 뉴런을 제외하고 출력 뉴런 제외)은 일시적으로 \"탈락\"되는 확률을 가지므로 이 훈련 단계에서 완전히 무시됩니다. 다음 단계에서 활성화 될 수 있습니다 (그림 11-9 참조). \n",
    "\n",
    "- 하이퍼 매개 변수 p는 드롭 아웃 비율이라고 하며 일반적으로 50%로 설정됩니다. 훈련 후, 뉴런은 더 이상 떨어지지 않습니다. 그리고 그게 전부입니다 (우리가 아래에서 논의 할 기술적인 세부 사항 제외).\n",
    "\n",
    "![img11_1](img/ch11_11.png)\n",
    "\n",
    "- 드롭 아웃으로 훈련 된 뉴런은 이웃하는 뉴런과 함께 적응할 수 없습니다. 가능한 한 유용해야 합니다. 그들은 또한 단지 몇 개의 입력 뉴런에 과도하게 의존 할 수 없으며, 각각의 입력 뉴런에 주의를 기울여야 합니다. 그들은 입력의 사소한 변화에 덜 민감합니다. 결국 더 보편화된보다 견고한 네트워크를 얻게 됩니다.\n",
    "\n",
    "- Dropout의 힘을 이해하는 또 다른 방법은 각 훈련 단계에서 고유 한 신경 네트워크가 생성된다는 것을 인식하는 것입니다. \n",
    "\n",
    "- 각 뉴런은 존재하거나 부재할 수 있기 때문에 가능한 총 2N 개의 네트워크가 있습니다 (N은 드롭 가능한 뉴런의 총 수입니다). \n",
    "\n",
    "- 이것은 동일한 신경망이 두 번 샘플링되는 것은 사실상 불가능한 아주 큰 숫자입니다. 10,000 개의 훈련 단계를 거치면 10,000 개의 서로 다른 신경 네트워크 (각각 하나의 훈련 인스턴스 만 있는)를 훈련하게 됩니다. 이러한 신경망은 많은 가중치를 공유하기 때문에 분명히 독립적이지는 않지만 그럼에도 불구하고 모두 다릅니다. 결과 neural 네트워크는 이러한 모든 작은 신경 네트워크의 평균 앙상블로 볼 수 있습니다.\n",
    "\n",
    "- 작지만 중요한 기술적 세부 사항이 있습니다. p = 50이라고 가정하면 테스트 중에 뉴런은 훈련 기간 동안 (평균적으로) 입력 뉴런의 두 배에 연결됩니다. \n",
    "\n",
    "- 이 사실을 보완하기 위해 훈련 후 각 뉴런의 입력 연결 가중치를 0.5로 곱해야합니다. 그렇지 않은 경우, 각 뉴런은 네트워크가 훈련 된 것보다 약 2 배 큰 총 입력 신호를 얻게 될 것이므로 성능이 좋지 않을 것입니다. \n",
    "\n",
    "- 보다 일반적으로, 우리는 각 입력 연결 무게에 훈련 후 유지 확률 (1 - p)을 곱해야합니다. 양자 택일로, 우리는 각 뉴런의 출력을 훈련 중 유지 확률로 나눌 수 있습니다 (이 대안들은 완벽하게 동일하지 않지만 똑같이 잘 작동합니다).\n",
    "\n",
    "- TensorFlow를 사용하여 드롭 아웃을 구현하려면 입력 레이어와 모든 숨겨진 레이어의 출력에 dropout() 함수를 적용하면 됩니다. 훈련 중,이 기능은 임의로 항목을 떨어 뜨리고 (0으로 설정) 나머지 항목을 유지 확률로 나눕니다. 훈련 후에 이 함수는 아무것도 하지 않습니다. 다음 코드는 드롭 아웃 정규화를 3계층 신경망에 적용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의 :이 책은 `tf.layers.dropout()`(이 장을 쓸 때 없었던) 대신 `tf.contrib.layers.dropout()`을 사용합니다. contrib 모듈의 내용은 예고없이 변경되거나 삭제 될 수 있기 때문에 `tf.layers.dropout ()`을 사용하는 것이 더 바람직합니다. `tf.layers.dropout()`함수는 약간의 차이점을 제외하면`tf.contrib.layers.dropout()`함수와 거의 동일합니다. 가장 중요한 것은:\n",
    "* keep 확률 (`keep_prob`)보다는 dropout rate (`rate`)를 지정해야 합니다. 여기서 `rate`는 간단하게 `1-keep_prob`와 같습니다.\n",
    "* is_training 매개 변수의 이름이 `training`으로 변경되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9205\n",
      "1 Test accuracy: 0.9418\n",
      "2 Test accuracy: 0.9486\n",
      "3 Test accuracy: 0.9513\n",
      "4 Test accuracy: 0.9554\n",
      "5 Test accuracy: 0.9569\n",
      "6 Test accuracy: 0.9617\n",
      "7 Test accuracy: 0.9597\n",
      "8 Test accuracy: 0.9615\n",
      "9 Test accuracy: 0.9674\n",
      "10 Test accuracy: 0.9655\n",
      "11 Test accuracy: 0.9656\n",
      "12 Test accuracy: 0.9685\n",
      "13 Test accuracy: 0.9664\n",
      "14 Test accuracy: 0.9657\n",
      "15 Test accuracy: 0.9666\n",
      "16 Test accuracy: 0.9684\n",
      "17 Test accuracy: 0.9678\n",
      "18 Test accuracy: 0.9688\n",
      "19 Test accuracy: 0.9677\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 물론 이전에 일괄 정규화를 수행한 것처럼 학습 할 때 is_training을 True로 설정하고 테스트 할 때는 False로 설정해야합니다.\n",
    "\n",
    "- 모델이 오버 피팅 (overfitting) 상태라는 것을 알게되면 드롭 아웃 비율을 높일 수 있습니다 (즉, keep_prob 하이퍼 파라미터를 줄입니다). 반대로 모델이 훈련 세트에 부합하지 않는 경우 드롭 아웃 비율을 줄여야 합니다 (예 : keep_prob 증가). 또한 대형 레이어의 드롭 아웃 속도를 높이고 작은 레이어의 드롭 아웃 속도를 줄일 수도 있습니다.\n",
    "\n",
    "- 드롭 아웃은 컨버전스를 상당히 늦추는 경향이 있지만 일반적으로 제대로 조정하면 더 나은 모델이됩니다. 일반적으로 추가 시간과 노력이 필요할 것입니다.\n",
    "\n",
    "- Dropconnect는 개별 연결이 전체 뉴런보다 무작위로 삭제되는 Dropout의 변형입니다. 일반적으로 드롭 아웃이 더 잘 수행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-norm regularization\n",
    "\n",
    "- 신경망에서 널리 사용되는 또 다른 정규화 기법은 max-norm regularization이라고 불린다. 각 뉴런에 대해, 입력 연결의 가중치를 제한한다. 여기서 r은 최대 norm hyperparameter이고 r을 줄이면 정규화 양이 늘어나고 overfitting을 줄일 수 있습니다. 최대 정규화는 일괄 정규화를 사용하지 않는 경우 소실 / 폭발 그라디언트 문제를 완화하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "- TensorFlow는 기성품 Max-norm 정규식을 제공하지 않지만 구현하기가 어렵지 않습니다. 다음 코드는 각 행 벡터의 최대 표준 값이 1.0이되도록 weight 변수를 두 번째 축을 따라 잘라내는 clip_weights 노드를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음으로, 첫 번째 숨겨진 레이어의 가중치를 처리하고 clip_by_norm() 함수를 사용하여 클리핑 된 가중치를 계산하는 연산을 만듭니다. 그런 다음 weipped 변수를 클리핑 된 가중치를 할당하는 할당 연산을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And now we can train the model. It's pretty much as usual, except that right after running the training_op, we run the clip_weights and clip_weights2 operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9519\n",
      "1 Test accuracy: 0.9673\n",
      "2 Test accuracy: 0.972\n",
      "3 Test accuracy: 0.9751\n",
      "4 Test accuracy: 0.9751\n",
      "5 Test accuracy: 0.9766\n",
      "6 Test accuracy: 0.9767\n",
      "7 Test accuracy: 0.9792\n",
      "8 Test accuracy: 0.9789\n",
      "9 Test accuracy: 0.98\n",
      "10 Test accuracy: 0.9803\n",
      "11 Test accuracy: 0.981\n",
      "12 Test accuracy: 0.9819\n",
      "13 Test accuracy: 0.9803\n",
      "14 Test accuracy: 0.982\n",
      "15 Test accuracy: 0.9819\n",
      "16 Test accuracy: 0.983\n",
      "17 Test accuracy: 0.9826\n",
      "18 Test accuracy: 0.982\n",
      "19 Test accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):  # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)       # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,       # not shown\n",
    "                                            y: mnist.test.labels})      # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                        # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 구현은 간단하고 잘 작동하지만 약간 지저분합니다. 더 나은 방법은 max_norm_regularizer () 함수를 정의하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런 다음이 함수를 호출하여 원하는 최대 임계 값을 얻을 수 있습니다. 숨겨진 레이어를 만들 때이 정규 표현식을 kernel_regularizer 인수에 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 트레이닝 작업 후에 가중치 클리핑 작업을 실행해야한다는 점을 제외하면 트레이닝은 평소와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최대 표준 정규화는 전체 손실 함수에 정규화 손실 기간을 추가 할 필요가 없으므로 max_norm() 함수는 없음을 반환합니다.\n",
    "\n",
    "- 그러나 각 트레이닝 단계 후에도 여전히 clip_weights 작업을 실행할 수 있어야 하므로 핸들을 처리 할 수 있어야 합니다. 이것이 max_norm() 함수가 clip_weights 노드를 Max norm clipping 연산의 모음에 추가하는 이유입니다. 이러한 클리핑 작업을 가져와 각 훈련 단계 후에 실행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.953\n",
      "1 Test accuracy: 0.9657\n",
      "2 Test accuracy: 0.9706\n",
      "3 Test accuracy: 0.9747\n",
      "4 Test accuracy: 0.9761\n",
      "5 Test accuracy: 0.9758\n",
      "6 Test accuracy: 0.9753\n",
      "7 Test accuracy: 0.979\n",
      "8 Test accuracy: 0.9775\n",
      "9 Test accuracy: 0.979\n",
      "10 Test accuracy: 0.9799\n",
      "11 Test accuracy: 0.9798\n",
      "12 Test accuracy: 0.9807\n",
      "13 Test accuracy: 0.9799\n",
      "14 Test accuracy: 0.9805\n",
      "15 Test accuracy: 0.9813\n",
      "16 Test accuracy: 0.981\n",
      "17 Test accuracy: 0.9813\n",
      "18 Test accuracy: 0.9815\n",
      "19 Test accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,     # not shown in the book\n",
    "                                            y: mnist.test.labels})    # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                      # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "- 마지막 정규화 기법은 기존의 정규화 기법을 사용하여 새로운 트레이닝 인스턴스를 생성하여 인위적으로 트레이닝 세트의 크기를 늘리는 것입니다. \n",
    "\n",
    "- 그러면 과도하게되는 현상이 줄어들어 정규화 기법이 됩니다. 트릭은 현실적인 훈련 인스턴스를 생성하는 것입니다. \n",
    "\n",
    "- 이상적으로 인간은 생성된 인스턴스와 그렇지 않은 인스턴스를 알 수 없어야 합니다. 또한 단순히 화이트 노이즈를 추가해도 도움이 되지 않습니다. 적용한 수정 내용을 학습할 수 있어야합니다 (화이트 노이즈는 아닙니다).\n",
    "\n",
    "- 예를 들어, 모델이 버섯 사진을 분류하기 위한 것이라면 다양한 세트로 훈련 세트의 모든 그림을 약간 이동, 회전 및 크기 조정하고 결과 세트를 훈련 세트에 추가 할 수 있습니다 (그림 11-10 참조). \n",
    "\n",
    "- 이것은 모델이 그림에서 버섯의 위치, 방향 및 크기에 보다 관대해 지도록 합니다. \n",
    "\n",
    "- 조명 조건에 대한 모델의 내성을 높이려면 다양한 대비를 사용하여 많은 이미지를 생성 할 수 있습니다. 버섯이 대칭이라면 그림을 수평으로 뒤집을 수도 있습니다. 이러한 변환을 결합하면 교육 세트의 크기를 크게 늘릴 수 있습니다.\n",
    "\n",
    "![img11_1](img/ch11_12.png)\n",
    "\n",
    "- 저장 공간과 네트워크 대역폭을 낭비하기보다는 훈련 중에 즉석에서 훈련 인스턴스를 생성하는 것이 좋습니다. \n",
    "\n",
    "- TensorFlow는 조 변경 (회전), 회전, 크기 조정, 뒤집기, 자르기, 밝기, 대비, 채도 및 색조 조정과 같은 몇 가지 이미지 조작 작업을 제공합니다 (자세한 내용은 API 문서를 참조하십시오). 따라서 이미지 데이터 세트의 데이터 기능을 쉽게 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical guidelines\n",
    "\n",
    "- 물론, 유사한 문제를 해결할 수 있는 신경망을 발견 할 수 있다면 미리 훈련 된 신경망의 일부를 재사용 해야 합니다.\n",
    "\n",
    "- 이 기본 구성을 조정해야 할 수도 있습니다.\n",
    "\n",
    "![img11_1](img/ch11_13.png)\n",
    "\n",
    "- 좋은 학습률을 찾을 수 없다면 (수렴 속도가 너무 느리고 훈련 속도가 빨라지 고 수렴은 빠르지 만 네트워크의 정확도는 최적임) 지수 감소와 같은 학습 일정을 추가 할 수 있습니다.\n",
    "\n",
    "- 훈련 세트가 너무 작으면 데이터 증가를 구현할 수 있습니다.\n",
    "\n",
    "- 희소 모델이 필요한 경우 믹스에 정규화를 추가 할 수 있습니다 (그리고 선택적으로 트레이닝 후 작은 가중치를 제로화 할 수 있음). \n",
    "\n",
    "- 더 희박한 모델이 필요하다면, 정규화와 함께 Adam 최적화 대신 FTRL을 사용해 볼 수 있습니다. 런타임에 번개가 빠른 모델이 필요한 경우 일괄 정규화를 삭제하고 Leaky ReLU로 ELU 활성화 함수를 대체 할 수 있습니다. 스파스 모델을 사용하면 도움이 됩니다.\n",
    "\n",
    "- 이 지침에 따라 이제는 아주 깊은 그물을 훈련 할 준비가 되었습니다. 단일 기계를 사용하는 경우 훈련을 완료하는 데 며칠 또는 몇 달 동안 기다려야 할 수 있습니다. 다음 장에서는 배포 된 TensorFlow를 사용하여 여러 서버와 GPU에서 모델을 교육하고 실행하는 방법에 대해 설명합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
